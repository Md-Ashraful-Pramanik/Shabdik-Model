{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Char-level model with simple LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constant "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Alv8TyT74vOa"
      },
      "outputs": [],
      "source": [
        "# For PC assign 0 for colab assign 1\n",
        "PC_OR_COLAB = 1\n",
        "\n",
        "# Resolve the base path depending on your running environment\n",
        "colab_base = '/content/drive/MyDrive/ashraful/paper-1/'\n",
        "pc_base = './'\n",
        "\n",
        "if PC_OR_COLAB == 1:\n",
        "    base = colab_base\n",
        "else:\n",
        "    base = pc_base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Vv-ivgYE4vOi"
      },
      "outputs": [],
      "source": [
        "new_dataset_path = base + 'dataset/data.txt'\n",
        "dataset_paths = [new_dataset_path]\n",
        "\n",
        "input_tokenizer_dir = base + 'dataset/input-tokenizer_char.pickle'\n",
        "target_tokenizer_dir = base + 'dataset/target-tokenizer_char.pickle'\n",
        "\n",
        "model_weights_path = base + 'saved-weights/char-level-4/weights'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqOiJuZK4vOq",
        "outputId": "3e2e5b2c-28c0-4985-8f69-4381ed0328c5"
      },
      "outputs": [],
      "source": [
        "# If you are using colab then this is meaningful\n",
        "if PC_OR_COLAB == 1:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G4ASKc-qyIbF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import urllib3\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udq3BXq10NhZ"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "imc3M7Wb9BQj"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self):\n",
        "        self.inp_lang_tokenizer = None\n",
        "        self.targ_lang_tokenizer = None\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "\n",
        "    def create_dataset(self):\n",
        "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
        "        lines = list()\n",
        "\n",
        "        for path in dataset_paths:\n",
        "            lines.extend(\n",
        "                io.open(path, encoding='UTF-8').read().strip().split('\\n'))\n",
        "\n",
        "        # lines = list(lines)\n",
        "        lines.sort()\n",
        "        print(len(lines))\n",
        "\n",
        "        word_pairs = [[[char for char in '<' + w.replace('ঃ\\n', '').replace(\n",
        "            '\\n', '') + '>'] for w in l.split(',')] for l in lines]\n",
        "\n",
        "        print(word_pairs[0][0])\n",
        "        print(word_pairs[0][1])\n",
        "\n",
        "        return zip(*word_pairs)\n",
        "\n",
        "    # Step 3 and Step 4\n",
        "    def tokenize(self, lang, lang_tokenizer=None, maxlen=20):\n",
        "        if lang_tokenizer is None:\n",
        "            lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "            lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',\n",
        "                                                               maxlen=maxlen, truncating='post')\n",
        "\n",
        "        return tensor, lang_tokenizer\n",
        "\n",
        "    def load_dataset(self):\n",
        "        # creating cleaned input, output pairs\n",
        "        self.retrieve_tokenizer()\n",
        "        inp_lang, targ_lang = self.create_dataset()\n",
        "\n",
        "        input_tensor, inp_lang_tokenizer = self.tokenize(\n",
        "            inp_lang, self.inp_lang_tokenizer)\n",
        "        target_tensor, targ_lang_tokenizer = self.tokenize(\n",
        "            targ_lang, self.targ_lang_tokenizer)\n",
        "\n",
        "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "    def retrieve_tokenizer(self):\n",
        "\n",
        "        try:\n",
        "            with open(input_tokenizer_dir, mode='rb') as data_file:\n",
        "                self.inp_lang_tokenizer = pickle.load(data_file)\n",
        "\n",
        "        except:\n",
        "            print(\"Not found input tokenizer\")\n",
        "            exit(1)\n",
        "\n",
        "        try:\n",
        "            with open(target_tokenizer_dir, mode='rb') as data_file:\n",
        "                self.targ_lang_tokenizer = pickle.load(data_file)\n",
        "\n",
        "        except:\n",
        "            print(\"Not found target tokenizer\")\n",
        "            exit(1)\n",
        "\n",
        "        # print(len(inp_lang_tokenizer.word_index))\n",
        "        # print(len(targ_lang_tokenizer.word_index))\n",
        "        return True\n",
        "\n",
        "    def call(self, BATCH_SIZE):\n",
        "        # if self.retrieve_data() == False:\n",
        "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = \\\n",
        "            self.load_dataset()\n",
        "\n",
        "        print(\"Input tensor\", input_tensor.shape)\n",
        "        print(\"Output tensor\", target_tensor.shape)\n",
        "\n",
        "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n",
        "            train_test_split(input_tensor, target_tensor,\n",
        "                             test_size=0.2, random_state=4651)\n",
        "\n",
        "        print(input_tensor_train.shape, target_tensor_train.shape)\n",
        "        print(input_tensor_train[500])\n",
        "        print(input_tensor_val[500])\n",
        "\n",
        "        BUFFER_SIZE = len(input_tensor_train)\n",
        "        self.train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (input_tensor_train, target_tensor_train))\n",
        "        self.train_dataset = self.train_dataset.shuffle(\n",
        "            BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        self.val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (input_tensor_val, target_tensor_val))\n",
        "        self.val_dataset = self.val_dataset.batch(\n",
        "            BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        return self.inp_lang_tokenizer, self.targ_lang_tokenizer, self.train_dataset, self.val_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPv0BItC9wEz",
        "outputId": "0fed2177-faa3-4518-f556-d786e353f6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2402977\n",
            "['<', 'a', '>']\n",
            "['<', 'অ', '>']\n",
            "Input tensor (2402977, 20)\n",
            "Output tensor (2402977, 20)\n",
            "(1922381, 20) (1922381, 20)\n",
            "[ 1 20  5  8  7 14  7  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[ 1  3 27  3  9  7  3  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "30037 7509 28 63\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset_creator = Dataset()\n",
        "inp_lang, targ_lang, train_dataset, val_dataset = dataset_creator.call(\n",
        "    BATCH_SIZE)\n",
        "\n",
        "print(len(train_dataset), len(val_dataset), len(\n",
        "    inp_lang.word_index), len(targ_lang.word_index))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLXYtWdZBW6a"
      },
      "source": [
        "### Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uFUu1izSBVIy"
      },
      "outputs": [],
      "source": [
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "steps_per_epoch = len(train_dataset)//BATCH_SIZE\n",
        "\n",
        "embedding_dims = 32\n",
        "rnn_units = 256\n",
        "dense_units = 256\n",
        "\n",
        "Tx = 20\n",
        "Ty = 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Swb9mvflqsQ"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJXNQ4Vh26t9",
        "outputId": "f01504ef-82b8-4271-e928-d0801ae66332"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1ce805df50>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer1 = tf.keras.layers.LSTM(\n",
        "            rnn_units, return_sequences=True)\n",
        "        self.encoder_rnnlayer2 = tf.keras.layers.LSTM(rnn_units,\n",
        "                                                      return_sequences=True,\n",
        "                                                      return_state=True)\n",
        "        self.encoder_norm = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        # Decoder\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        self.attention_mechanism = self.build_attention_mechanism(\n",
        "            dense_units, None, BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell,\n",
        "                                                sampler=self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "        self.attention_mechanism.memory_initialized\n",
        "        self.decoder_embedding_matrix = None\n",
        "\n",
        "    def initialize_initial_state(self):\n",
        "        self.initial_state = [\n",
        "            tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n",
        "\n",
        "    def build_attention_mechanism(self, units, memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units,\n",
        "                                          memory=memory,\n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        # return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decoder rnn cell\n",
        "    def build_rnn_cell(self, batch_size):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "\n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_size,\n",
        "                                                                dtype=Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(\n",
        "            cell_state=encoder_state)\n",
        "        return decoder_initial_state\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_input, decoder_input = inputs\n",
        "\n",
        "        x = self.encoder_embedding(encoder_input)\n",
        "        x = self.encoder_rnnlayer1(x)\n",
        "        x = self.encoder_norm(x, training=training)\n",
        "        a, a_tx, c_tx = self.encoder_rnnlayer2(x)\n",
        "\n",
        "        decoder_emb_inp = self.decoder_embedding(decoder_input)\n",
        "        self.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = self.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                 encoder_state=[\n",
        "                                                                     a_tx, c_tx],\n",
        "                                                                 Dtype=tf.float32)\n",
        "\n",
        "        outputs, _, _ = self.decoder(decoder_emb_inp,\n",
        "                                     initial_state=decoder_initial_state,\n",
        "                                     sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def evaluate(self, inputs, beam_width=3):\n",
        "        if self.decoder_embedding_matrix is None:\n",
        "            self.decoder_embedding_matrix = tf.train.load_variable(\n",
        "                model_weights_path, 'decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "            print(self.decoder_embedding_matrix.shape)\n",
        "\n",
        "        inference_batch_size = inputs.shape[0]\n",
        "\n",
        "        x = self.encoder_embedding(inputs)\n",
        "        x = self.encoder_rnnlayer1(x)\n",
        "        x = self.encoder_norm(x, training=False)\n",
        "        enc_out, enc_h, enc_c = self.encoder_rnnlayer2(x)\n",
        "\n",
        "        dec_h = enc_h\n",
        "        # dec_c = enc_c\n",
        "\n",
        "        start_tokens = tf.fill([inference_batch_size],\n",
        "                               targ_lang.word_index['<'])\n",
        "        end_token = targ_lang.word_index['>']\n",
        "\n",
        "        enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
        "        self.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "        hidden_state = tfa.seq2seq.tile_batch(\n",
        "            [enc_h, enc_c], multiplier=beam_width)\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=beam_width * inference_batch_size,\n",
        "                                                                dtype=tf.float32)\n",
        "        decoder_initial_state = decoder_initial_state.clone(\n",
        "            cell_state=hidden_state)\n",
        "\n",
        "        decoder_instance = tfa.seq2seq.BeamSearchDecoder(self.rnn_cell,\n",
        "                                                         beam_width=beam_width,\n",
        "                                                         output_layer=self.dense_layer)\n",
        "        decoder_instance.maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "        outputs, final_state, sequence_lengths = decoder_instance(self.decoder_embedding_matrix,\n",
        "                                                                  start_tokens=start_tokens,\n",
        "                                                                  end_token=end_token,\n",
        "                                                                  initial_state=decoder_initial_state)\n",
        "\n",
        "        final_outputs = tf.transpose(outputs.predicted_ids, perm=(0, 2, 1))\n",
        "        beam_scores = tf.transpose(\n",
        "            outputs.beam_search_decoder_output.scores, perm=(0, 2, 1))\n",
        "\n",
        "        return final_outputs.numpy(), beam_scores.numpy()\n",
        "\n",
        "\n",
        "model = MyModel(vocab_inp_size, vocab_tar_size, embedding_dims, rnn_units)\n",
        "model.load_weights(filepath=model_weights_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfJTg36aCckr"
      },
      "source": [
        "### Optimizer and Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yc7yp0FWEedf"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4LrCHr3N9RWf"
      },
      "outputs": [],
      "source": [
        "def get_bangla(array):\n",
        "    bangla_list = list(\n",
        "        map(lambda x: targ_lang.index_word[x] if x != 0 else '', array))\n",
        "    bangla_list.append('>')\n",
        "    return \"\".join(bangla_list[0:bangla_list.index('>')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0pT6hAEFEhVZ"
      },
      "outputs": [],
      "source": [
        "sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(y_pred, y):\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    # output 0 for y=0 else output 1\n",
        "    mask = tf.logical_not(tf.math.equal(y, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask * loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def acc_function(pred, real):\n",
        "    pred = tf.reshape(pred, [pred.shape[0], 19, pred.shape[2]])\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    pred = tf.cast(pred, dtype=real.dtype)\n",
        "    pred = list(map(lambda x: get_bangla(x), pred.numpy()))\n",
        "    real = list(map(lambda x: get_bangla(x), real.numpy()))\n",
        "    accuracies = tf.equal(real, pred).numpy()\n",
        "\n",
        "    return accuracies.sum() / accuracies.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8vdgaZFAah"
      },
      "source": [
        "### One step of training on a batch using Teacher Forcing technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Nm3HkY9nExNB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_step(input_batch, output_batch):\n",
        "    # initialize loss = 0\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:, :-1]  # ignore <end>\n",
        "        # compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:, 1:]  # ignore <start>\n",
        "\n",
        "        outputs = model([input_batch, decoder_input], True)\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        # Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "        acc = acc_function(logits, decoder_output)\n",
        "\n",
        "    # Returns the list of all layer variables / weights.\n",
        "    variables = model.trainable_variables\n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients, variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss, acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_GRb1JHp6b"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OT9njTi-pdB"
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "dataset = train_dataset\n",
        "steps_per_epoch = len(dataset)\n",
        "print(steps_per_epoch)\n",
        "max_acc = .00\n",
        "\n",
        "for epoch in range(start, EPOCHS+start):\n",
        "    start = time.time()\n",
        "\n",
        "    # encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        # print(inp.shape, targ.shape)\n",
        "        batch_loss, batch_acc = train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "        total_acc += batch_acc\n",
        "\n",
        "        if batch % 1000 == 0:\n",
        "            print(\n",
        "                f'Epoch {epoch + 1} Upto Batch {batch+1} Loss {total_loss / (batch+1):.4f} Accuracy {total_acc / (batch+1):.4f}')\n",
        "            # model.save_weights(filepath=model_weights_path)\n",
        "            # break\n",
        "\n",
        "    # break\n",
        "\n",
        "    acc = total_acc / steps_per_epoch\n",
        "    if acc > max_acc:\n",
        "        max_acc = acc\n",
        "        model.save_weights(filepath=model_weights_path)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f} Accuracy {total_acc / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOJtMbjZsAuh"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aW3pmxqJ4vPV"
      },
      "outputs": [],
      "source": [
        "def get_bangla(array):\n",
        "    bangla_list = list(\n",
        "        map(lambda x: targ_lang.index_word[x] if x != 0 else '', array))\n",
        "    bangla_list.append('>')\n",
        "    return \"\".join(bangla_list[0:bangla_list.index('>')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_bWQb_h44vPV"
      },
      "outputs": [],
      "source": [
        "# Evaluate char-level train\n",
        "def calculate_acc(dataset):\n",
        "    beam_width = 10\n",
        "    correct_count = np.array([0]*4)\n",
        "    total_count = 0\n",
        "    steps_per_epoch = len(dataset)\n",
        "    print(steps_per_epoch)\n",
        "    # exit(0)\n",
        "    start = time.time()\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        # outputs, scores = beam_evaluate(inp, beam_width=beam_width)\n",
        "        outputs, scores = model.evaluate(inp, beam_width=beam_width)\n",
        "        # print(targ.shape)\n",
        "        targ = list(map(get_bangla, targ.numpy()))\n",
        "        targ = list(map(lambda x: x.replace('<', ''), targ))\n",
        "        # print(targ)\n",
        "        outputs = [list(map(get_bangla, output)) for output in outputs]\n",
        "        # print(outputs)\n",
        "\n",
        "        for i in range(len(targ)):\n",
        "            if targ[i] == outputs[i][0]:\n",
        "                correct_count[0] += 1\n",
        "            if targ[i] in outputs[i][0:3]:\n",
        "                correct_count[1] += 1\n",
        "            if targ[i] in outputs[i][0:5]:\n",
        "                correct_count[2] += 1\n",
        "            if targ[i] in outputs[i]:\n",
        "                correct_count[3] += 1\n",
        "            total_count += 1\n",
        "\n",
        "    print(f'Total size {total_count}')\n",
        "    print(f'Acc@1 : {((correct_count[0]/total_count))*100:.2f} %')\n",
        "    print(f'Acc@3 : {((correct_count[1]/total_count))*100:.2f} %')\n",
        "    print(f'Acc@5 : {((correct_count[2]/total_count))*100:.2f} %')\n",
        "    print(f'Acc@10: {((correct_count[3]/total_count))*100:.2f} %')\n",
        "    print(f'Time taken: {(time.time() - start):.2f} s\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calculate_acc(train_dataset)\n",
        "calculate_acc(val_dataset)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DqFYB-g1HM5o"
      ],
      "name": "char-level-model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
