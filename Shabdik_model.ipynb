{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Char-level model"]},{"cell_type":"markdown","metadata":{},"source":["### Constant "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Alv8TyT74vOa"},"outputs":[],"source":["# For PC assign 0 for colab assign 1\n","PC_OR_COLAB = 1\n","\n","# Resolve the base path depending on your running environment\n","colab_base = '/content/drive/MyDrive/ashraful/paper-1/'\n","pc_base = './'\n","\n","if PC_OR_COLAB == 1:\n","    base = colab_base\n","else:\n","    base = pc_base\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vv-ivgYE4vOi"},"outputs":[],"source":["ALL = 0\n","WORD_LEVEL = 1\n","ENCODER_DECODER = 2\n","\n","test_dataset_path = base + 'dataset/test-dataset.txt'\n","dataset_paths = [test_dataset_path]\n","\n","input_tokenizer_dir = base + 'dataset/input-tokenizer_char.pickle'\n","target_tokenizer_dir = base + 'dataset/target-tokenizer_char.pickle'\n","\n","# char-level-model paths\n","model_weights_path = base + 'models/LSTM/char-level-model/weights'\n","input_tokenizer_dir_char = base + 'dataset/input-tokenizer_char.pickle'\n","target_tokenizer_dir_char = base + 'dataset/target-tokenizer_char.pickle'\n","\n","# word-level-model paths\n","saved_model_word = base + 'word-level-model/word-level.h5'\n","input_tokenizer_dir_word = base + 'dataset/input-tokenizer_word.pickle'\n","target_tokenizer_dir_word = base + 'dataset/target-tokenizer_word.pickle'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqOiJuZK4vOq","outputId":"3e2e5b2c-28c0-4985-8f69-4381ed0328c5"},"outputs":[],"source":["# If you are using colab then this is meaningful\n","if PC_OR_COLAB == 1:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !pip install tensorflow-addons\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3349,"status":"ok","timestamp":1642227707058,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"G4ASKc-qyIbF","outputId":"f3186c72-8970-4df8-db03-137e0094b279"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pickle\n","import numpy as np\n","import urllib3\n","import shutil\n","import zipfile\n","import itertools\n","from threading import Thread\n","import random\n"]},{"cell_type":"markdown","metadata":{"id":"TOGq0hRk9tad"},"source":["### Create Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1642227713792,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"IpmCsSP3Vyfo","outputId":"4f334e4e-2169-499d-f83b-4cd4932473de"},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n"]}],"source":["# Splitting dataset into train test\n","lines = list()\n","\n","for path in dataset_paths:\n","    lines.extend(io.open(path, encoding='UTF-8').read().strip().split('\\n'))\n","\n","# lines = list(lines)\n","lines.sort()\n","print(len(lines))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1642227715591,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"QZdyWFvC0_dQ","outputId":"56ca2dd4-548d-42fd-dbe5-f0f7aac473d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["abagprobon\n","আবেগপ্রবণ\n"]}],"source":["word_pairs = [[w for w in l.split(',')] for l in lines]\n","\n","print(word_pairs[0][0])\n","print(word_pairs[0][1])\n","\n","inp_lang, targ_lang = zip(*word_pairs)\n","inp_lang_train, inp_lang_val, targ_lang_train, targ_lang_val = \\\n","    train_test_split(inp_lang, targ_lang, test_size=0.2, random_state=4651)\n"]},{"cell_type":"markdown","metadata":{"id":"fLXYtWdZBW6a"},"source":["### Char-Level Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nV0XTs8YV0xk"},"outputs":[],"source":["# Retrieving tokenizer for char-level\n","with open(input_tokenizer_dir_char, mode='rb') as data_file:\n","    inp_lang_tokenizer_char = pickle.load(data_file)\n","with open(target_tokenizer_dir_char, mode='rb') as data_file:\n","    tar_lang_tokenizer_char = pickle.load(data_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1642227722522,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"uFUu1izSBVIy","outputId":"60bf3b79-9228-403f-a7f3-93aaaec69f50"},"outputs":[],"source":["vocab_inp_size = len(inp_lang_tokenizer_char.word_index) + 1\n","vocab_tar_size = len(tar_lang_tokenizer_char.word_index) + 1\n","max_length_input = 20\n","max_length_output = 20\n","\n","# print(inp_lang_tokenizer_char.word_index)\n","# print(tar_lang_tokenizer_char.word_index)\n","\n","embedding_dims = 32\n","rnn_units = 256\n","dense_units = 256\n","Dtype = tf.float32\n","\n","Tx = 20\n","Ty = 20\n","\n","# print(vocab_inp_size, vocab_tar_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1642227724467,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"CJXNQ4Vh26t9","outputId":"a060e76c-6a6c-4029-c177-699ac69e9c66"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8be0504890>"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["\n","class MyModel(tf.keras.Model):\n","    def __init__(self, input_vocab_size, output_vocab_size, embedding_dims, rnn_units):\n","        super().__init__()\n","        # Encoder\n","        self.input_vocab_size = input_vocab_size\n","        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n","                                                           output_dim=embedding_dims)\n","        self.encoder_rnnlayer1 = tf.keras.layers.Bidirectional(\n","            tf.keras.layers.LSTM(rnn_units, return_sequences=True))\n","        self.encoder_rnnlayer2 = tf.keras.layers.LSTM(rnn_units,\n","                                                      return_sequences=True,\n","                                                      return_state=True)\n","        self.encoder_norm = tf.keras.layers.BatchNormalization()\n","\n","        # Decoder\n","        self.output_vocab_size = output_vocab_size\n","        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n","                                                           output_dim=embedding_dims)\n","        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n","        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n","        # Sampler\n","        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n","        self.attention_mechanism = self.build_attention_mechanism(\n","            dense_units, None, BATCH_SIZE*[Tx])\n","        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n","        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell,\n","                                                sampler=self.sampler,\n","                                                output_layer=self.dense_layer)\n","\n","        self.attention_mechanism.memory_initialized\n","        self.decoder_embedding_matrix = None\n","\n","    def initialize_initial_state(self):\n","        self.initial_state = [\n","            tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n","\n","    def build_attention_mechanism(self, units, memory, memory_sequence_length):\n","        return tfa.seq2seq.LuongAttention(units,\n","                                          memory=memory,\n","                                          memory_sequence_length=memory_sequence_length)\n","        # return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n","\n","    # wrap decoder rnn cell\n","    def build_rnn_cell(self, batch_size):\n","        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n","                                                attention_layer_size=dense_units)\n","        return rnn_cell\n","\n","    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):\n","        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_size,\n","                                                                dtype=Dtype)\n","        decoder_initial_state = decoder_initial_state.clone(\n","            cell_state=encoder_state)\n","        return decoder_initial_state\n","\n","    def call(self, inputs, training=False):\n","        encoder_input, decoder_input = inputs\n","\n","        x = self.encoder_embedding(encoder_input)\n","        x = self.encoder_rnnlayer1(x)\n","        x = self.encoder_norm(x, training=training)\n","        a, a_tx, c_tx = self.encoder_rnnlayer2(x)\n","\n","        decoder_emb_inp = self.decoder_embedding(decoder_input)\n","        self.attention_mechanism.setup_memory(a)\n","        decoder_initial_state = self.build_decoder_initial_state(BATCH_SIZE,\n","                                                                 encoder_state=[\n","                                                                     a_tx, c_tx],\n","                                                                 Dtype=tf.float32)\n","\n","        outputs, _, _ = self.decoder(decoder_emb_inp,\n","                                     initial_state=decoder_initial_state,\n","                                     sequence_length=BATCH_SIZE*[Ty-1])\n","\n","        return outputs\n","\n","    def evaluate(self, inputs, beam_width=3):\n","        if self.decoder_embedding_matrix is None:\n","            self.decoder_embedding_matrix = tf.train.load_variable(\n","                model_weights_path, 'decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n","            print(self.decoder_embedding_matrix.shape)\n","\n","        inference_batch_size = inputs.shape[0]\n","        # print(inputs.shape)\n","\n","        x = self.encoder_embedding(inputs)\n","        # x = tf.one_hot(inputs, depth=self.input_vocab_size)\n","        x = self.encoder_rnnlayer1(x)\n","        x = self.encoder_norm(x, training=False)\n","        enc_out, enc_h, enc_c = self.encoder_rnnlayer2(x)\n","\n","        dec_h = enc_h\n","        # dec_c = enc_c\n","\n","        start_tokens = tf.fill([inference_batch_size],\n","                               targ_lang.word_index['<'])\n","        end_token = targ_lang.word_index['>']\n","\n","        enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n","        self.attention_mechanism.setup_memory(enc_out)\n","\n","        hidden_state = tfa.seq2seq.tile_batch(\n","            [enc_h, enc_c], multiplier=beam_width)\n","        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=beam_width * inference_batch_size,\n","                                                                dtype=tf.float32)\n","        decoder_initial_state = decoder_initial_state.clone(\n","            cell_state=hidden_state)\n","\n","        decoder_instance = tfa.seq2seq.BeamSearchDecoder(self.rnn_cell,\n","                                                         beam_width=beam_width,\n","                                                         output_layer=self.dense_layer)\n","        decoder_instance.maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n","\n","        outputs, final_state, sequence_lengths = decoder_instance(self.decoder_embedding_matrix,\n","                                                                  start_tokens=start_tokens,\n","                                                                  end_token=end_token,\n","                                                                  initial_state=decoder_initial_state)\n","\n","        final_outputs = tf.transpose(outputs.predicted_ids, perm=(0, 2, 1))\n","        beam_scores = tf.transpose(\n","            outputs.beam_search_decoder_output.scores, perm=(0, 2, 1))\n","\n","        return final_outputs.numpy(), beam_scores.numpy()\n","\n","\n","model = MyModel(vocab_inp_size, vocab_tar_size, embedding_dims, rnn_units)\n","model.load_weights(filepath=model_weights_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ltK44v2NFZq"},"outputs":[],"source":["def get_bangla(array):\n","    bangla_list = list(\n","        map(lambda x: tar_lang_tokenizer_char.index_word[x] if x != 0 else '', array))\n","    bangla_list.append('>')\n","    return \"\".join(bangla_list[0:bangla_list.index('>')])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNGAtxO8To0G"},"outputs":[],"source":["def preprocess_char(word):\n","    word = [[char for char in ('<' + word.rstrip().lstrip() + '>')]]\n","    word = inp_lang_tokenizer_char.texts_to_sequences(word)\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences(word,\n","                                                           padding='post',\n","                                                           maxlen=20,\n","                                                           truncating='post')\n","    # print(inputs)\n","    return tf.convert_to_tensor(inputs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2Gq5sniTfX6"},"outputs":[],"source":["def predict_for_char(english_word, result, k=10):\n","    # start = time.time()\n","    outputs, score = model.evaluate(preprocess_char(english_word), k)\n","    outputs = [list(map(get_bangla, output)) for output in outputs]\n","    result[0] = outputs[0]\n","\n","    # print(f'Time taken: {(time.time() - start)*1000:.2f} ms\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1642227732591,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"bnoE_X7FK2H8","outputId":"3e96d78a-18b4-4f8c-9b33-deff37fbfe6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['আমি', 'অমি', 'এমই', 'আমী', 'আমই', 'এমি', 'অমী', 'এমী', 'মী', 'মি']\n"]}],"source":["predictions = [None]\n","predict_for_char(\"ami\", predictions)\n","print(predictions[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["### Word-level"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1642227733776,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"mpZAusBPN7Uu","outputId":"2bc74246-58a1-4275-acb3-00ce66036783"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 12, 32)            928       \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 256)              164864    \n"," nal)                                                            \n","                                                                 \n"," dense_4 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                2080      \n","                                                                 \n"," dense_7 (Dense)             (None, 50001)             1650033   \n","                                                                 \n","=================================================================\n","Total params: 1,859,057\n","Trainable params: 1,859,057\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Loading word-level model\n","word_model = tf.keras.models.load_model(saved_model_word)\n","word_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nt1BPO84OWJE"},"outputs":[],"source":["# Retrieving tokenizer for word-level\n","with open(input_tokenizer_dir_word, mode='rb') as data_file:\n","    inp_lang_tokenizer_word = pickle.load(data_file)\n","with open(target_tokenizer_dir_word, mode='rb') as data_file:\n","    tar_lang_tokenizer_word = pickle.load(data_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2zrMEEwGW5C"},"outputs":[],"source":["def preprocess_word(word):\n","    word = [[char for char in ('<' + word.rstrip().lstrip() + '>')]]\n","    word = inp_lang_tokenizer_word.texts_to_sequences(word)\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences(word,\n","                                                           padding='post',\n","                                                           maxlen=12,\n","                                                           truncating='post')\n","\n","    return tf.convert_to_tensor(inputs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsOLCJZiFc3V"},"outputs":[],"source":["def predict_for_word(input, result, k=10):\n","    if len(input) > 10:\n","        result[0] = []\n","        return\n","\n","    predictions = tf.math.top_k(tf.reshape(\n","        word_model.predict(preprocess_word(input)), [-1]), k)\n","    values = predictions.values.numpy()\n","    indices = predictions.indices.numpy()\n","\n","    for i in range(1, k):\n","        # print(values[i-1]/values[i])\n","        if values[i - 1] / values[i] > 5 or values[i] <= 0.20:\n","            result[0] = (list(\n","                map(lambda x: tar_lang_tokenizer_word.index_word[x], indices[0:i])), values[0:i])\n","            return\n","\n","    result[0] = (\n","        list(map(lambda x: tar_lang_tokenizer_word.index_word[x], indices)), values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1372,"status":"ok","timestamp":1642227739509,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"TJfxOZDFQP3I","outputId":"f9b777f7-e8e7-471d-f340-54bb989b19ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["(['কুমীর'], array([0.80083203], dtype=float32))\n"]}],"source":["predictions = [None]\n","predict_for_word(\"kumir\", predictions)\n","print(predictions[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["## Shabdik-Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FWfdcqoOyGU"},"outputs":[],"source":["# Fixed mapping for single character word\n","single_word_dic = {\n","    'a': ['আ', 'া', 'অ', 'এ', 'ে', 'আঃ', 'া' + 'ঁ', 'আ' + 'ঁ'],\n","    'b': ['ব', 'ভ', '্' + 'ব', 'ব' + 'ি'],\n","    'c': ['চ', 'ছ', 'ক', 'ঁ', '্' + 'চ', '্' + 'ছ', 'স' + 'ি'],\n","    'd': ['ড', 'দ', 'ধ', 'দঃ', 'ডঃ', 'ঢ', '্' + 'ধ', 'ড' + 'ি', 'দ' + 'ি'],\n","    'e': ['এ', 'ে', 'আ', 'া', 'ই', 'ি', 'ঈ', 'ী', '্' + 'য' + 'া'],\n","    'f': ['ফ', 'প', '্' + 'ফ', '্' + 'প', 'এ' + 'ফ'],\n","    'g': ['গ', 'ঘ', 'য', 'জ', 'ং', 'ঞ', '্', 'জ' + 'ি'],\n","    'h': ['হ', '্', 'এ' + 'ই' + 'চ'],\n","    'i': ['ই', 'ি', 'ঈ', 'ী', 'ৈ', 'আ' + 'ই'],\n","    'j': ['জ', 'য', 'ঝ', 'জ্ব', '্' + 'জ', '্' + 'য', '্' + 'ঝ', 'জ' + 'ে'],\n","    'k': ['ক', 'খ', 'ক্ষ', '্' + 'ক', '্' + 'খ', 'ক' + 'ে'],\n","    'l': ['ল', '্' + 'ল', 'এ' + 'ল'],\n","    'm': ['ম', '্' + 'ম', 'এ' + 'ম'],\n","    'n': ['ন', 'ণ', 'ঙ', 'ঞ', 'ং', '্' + 'ন', '্' + 'ণ', '্' + 'ঙ', 'এ' + 'ন'],\n","    'o': ['ও', 'ঐ', 'ো', 'য়', 'ঃ', 'ওঁ', 'ঔ', 'ৌ', 'ৈ'],\n","    'p': ['প', 'ফ', '্' + 'প', '্' + 'ফ', 'প' + 'ি'],\n","    'q': ['ক', '্' + 'ক', 'ক' + 'ি' + 'উ'],\n","    'r': ['র', 'ঢ়', 'ড়', 'ঋ', 'ৃ', '্' + 'র', 'র' + '্', 'আ' + 'র'],\n","    's': ['স', 'শ', 'ষ', 'সঃ', '্' + 'স', '্' + 'শ', '্' + 'ষ', 'এ' + 'স'],\n","    't': ['ট', 'ঠ', 'ত', 'থ', 'তঁ', 'ৎ', '্' + 'ট', '্' + 'ঠ', 'ট' + 'ি'],\n","    'u': ['উ', 'ু', 'ঊ', 'ূ', 'উঃ', 'উঁ', 'ই' + 'উ'],\n","    'v': ['ভ', 'ব', 'ভঁ', '্' + 'ভ', '্' + 'ব', 'ভ' + 'ি'],\n","    'w': ['ও', 'ৌ', 'ওঃ', 'ওঁ', 'ড' + 'া' + 'ব' + 'ল' + 'ু'],\n","    'x': ['ক্স', 'এক্স'],\n","    'y': ['য়', 'ইয়', 'ই', 'ি', 'ঈ', 'ী', 'ে', 'ও' + 'য়' + 'া' + 'ই'],\n","    'z': ['য', 'জ', 'ঝ', '্' + 'য', '্' + 'জ', '্' + 'ঝ', 'জ' + 'এ' + 'ড', 'জ' + 'ি']\n","}\n","\n","for key in single_word_dic:\n","    for i in range(len(single_word_dic[key]), 9):\n","        single_word_dic[key].append('')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDzaMBTgPB7N"},"outputs":[],"source":["def predict(english_word, option=ALL, k=10):\n","    if type(english_word) != type(''):\n","        return []\n","\n","    english_word = english_word.lower()\n","\n","    if len(english_word) == 0:\n","        return []\n","\n","    # print(english_word)\n","\n","    if len(english_word) == 1:\n","        return single_word_dic[english_word][0:k]\n","\n","    word_result = [None]\n","    char_result = [None]\n","\n","    if option == WORD_LEVEL:\n","        if len(english_word) > 10:\n","            return []\n","        predict_for_word(english_word, word_result, k)\n","        # print(word_result)\n","    elif option == ENCODER_DECODER or (option == ALL and len(english_word) > 10):\n","        predict_for_char(english_word, char_result, k)\n","    else:\n","        t1 = Thread(target=predict_for_char, args=(\n","            english_word, char_result, k, ))\n","        t1.start()\n","        t2 = Thread(target=predict_for_word, args=(\n","            english_word, word_result, k, ))\n","        t2.start()\n","        t2.join()\n","        t1.join()\n","\n","    char_values = np.array([i*.1 for i in range(k, -1, -1)])\n","    char_values = char_values / np.sum(char_values)\n","    results = {}\n","\n","    # print(word_result[0])\n","    # print(char_result[0])\n","\n","    word_result = word_result[0]\n","    char_result = char_result[0]\n","\n","    # print(char_result)\n","    # print(word_result)\n","\n","    if word_result is not None:\n","        for i in range(len(word_result[1])):\n","            # if word_result[0][i] in results:\n","            results[word_result[0][i]] = word_result[1][i]\n","\n","    if char_result is not None:\n","        for i in range(len(char_result)):\n","            if char_result[i] in results:\n","                results[char_result[i]] += char_values[i]\n","            else:\n","                results[char_result[i]] = char_values[i]\n","\n","    return [key for (key, value) in sorted(results.items(), key=lambda x: x[1], reverse=True)[:k]]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_result(true, prediction):\n","    results = [0, 0, 0, 0]\n","    if prediction is None or len(prediction) == 0:\n","        return results\n","\n","    if true == prediction[0]:\n","        results[0] += 1\n","    if true in prediction[0:3]:\n","        results[1] += 1\n","    if true in prediction[0:5]:\n","        results[2] += 1\n","    if true in prediction:\n","        results[3] += 1\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start = time.time()\n","predictions = [predict(inp_lang[i], k=10) for i in range(len(inp_lang))]\n","combined_results = [predict(targ_lang[i], predictions[i])\n","                    for i in range(len(inp_lang))]\n","combined_results = np.array(combined_results)\n","print(\"combined\", combined_results / i)\n","print(f'Time taken for {i+1} examples: {(time.time() - start):.2f} s\\n')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TOJtMbjZsAuh"],"name":"Shabdik-model.ipynb","provenance":[{"file_id":"1f_vk_fPEFOP0cef3FJRaM2fBYCb3PM_-","timestamp":1642007457660},{"file_id":"1CRne9RSF7nF5KWhrvmnANA0BnTIvwh5T","timestamp":1634998819559},{"file_id":"1UxykXamm_aEkHzDEYZIVFEt5jxtYu1cn","timestamp":1624193880157},{"file_id":"https://github.com/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb","timestamp":1621616155912}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
