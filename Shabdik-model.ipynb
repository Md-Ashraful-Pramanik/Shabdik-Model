{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4730,"status":"ok","timestamp":1642227703716,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"Xo02O1JtJU5g","outputId":"824121a1-8c23-498e-9380-237133117af2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3349,"status":"ok","timestamp":1642227707058,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"G4ASKc-qyIbF","outputId":"f3186c72-8970-4df8-db03-137e0094b279"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}],"source":["!pip install tensorflow-addons\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pickle\n","import numpy as np\n","import urllib3\n","import shutil\n","import zipfile\n","import itertools\n","from threading import Thread\n","import random"]},{"cell_type":"markdown","metadata":{"id":"Udq3BXq10NhZ"},"source":["### Download File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-fvT1cC0Qsd"},"outputs":[],"source":["ALL             = 0\n","WORD_LEVEL      = 1\n","ENCODER_DECODER = 2\n","\n","colab_base = '/content/drive/MyDrive/ashraful/'\n","pc_base    = './'\n","base       = colab_base\n","\n","\n","# dataset_paths = [new_dataset_path, top_50k_word_file, top20k_path, top20k_path]\n","dataset_paths = [base + 'models/LSTM/test-dataset(1).txt']\n","test_dataset_path = base + 'models/LSTM/test-dataset.txt'\n","\n","# char-level-model paths\n","checkpoint_dir            = base + 'models/LSTM/char-level'\n","model_weights_path        = base + 'models/LSTM/char-level/weights'\n","input_tokenizer_dir_char  = base + 'models/LSTM/char-level/input-tokenizer.pickle'\n","target_tokenizer_dir_char = base + 'models/LSTM/char-level/target-tokenizer.pickle'\n","\n","# word-level-model paths\n","saved_model_dir_base      = base + 'models/LSTM/word-level-new/'\n","saved_model_word          = saved_model_dir_base + 'word-level.h5'\n","input_tokenizer_dir_word  = saved_model_dir_base + 'input-tokenizer.pickle'\n","target_tokenizer_dir_word = saved_model_dir_base + 'target-tokenizer.pickle'"]},{"cell_type":"markdown","metadata":{"id":"TOGq0hRk9tad"},"source":["### Create Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1642227713792,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"IpmCsSP3Vyfo","outputId":"4f334e4e-2169-499d-f83b-4cd4932473de"},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n"]}],"source":["# Splitting dataset into train test\n","lines = list()\n","\n","for path in dataset_paths:\n","    lines.extend(io.open(path, encoding='UTF-8').read().strip().split('\\n'))\n","\n","# lines = list(lines)\n","lines.sort()\n","print(len(lines))\n","\n","# word_pairs = [[[char for char in '<' + w.replace('ঃ\\n', '').replace('\\n', '') + '>'] for w in l.split(',')] for l in lines]\n","\n","# print(word_pairs[0][0])\n","# print(word_pairs[0][1])\n","\n","# inp_lang, targ_lang = zip(*word_pairs)\n","\n","# inp_lang_train, inp_lang_val, targ_lang_train, targ_lang_val = \\\n","#             train_test_split(inp_lang, targ_lang, test_size=0.2, random_state=4651)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1642227715591,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"QZdyWFvC0_dQ","outputId":"56ca2dd4-548d-42fd-dbe5-f0f7aac473d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["abagprobon\n","আবেগপ্রবণ\n"]}],"source":["word_pairs = [[w for w in l.split(',')] for l in lines]\n","\n","print(word_pairs[0][0])\n","print(word_pairs[0][1])\n","\n","inp_lang, targ_lang = zip(*word_pairs)\n","inp_lang_train, inp_lang_val, targ_lang_train, targ_lang_val = \\\n","            train_test_split(inp_lang, targ_lang, test_size=0.2, random_state=4651)\n","\n","# inp_lang_train  = inp_lang_train.to_numpy()\n","# inp_lang_val    = inp_lang_val.to_numpy()\n","# targ_lang_train = targ_lang_train.to_numpy()\n","# targ_lang_val   = targ_lang_val.to_numpy()"]},{"cell_type":"markdown","metadata":{"id":"fLXYtWdZBW6a"},"source":["### Char-Level Model Parameters "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nV0XTs8YV0xk"},"outputs":[],"source":["# Retrieving tokenizer for char-level\n","with open(input_tokenizer_dir_char, mode='rb') as data_file:\n","    inp_lang_tokenizer_char = pickle.load(data_file)\n","with open(target_tokenizer_dir_char, mode='rb') as data_file:\n","    tar_lang_tokenizer_char = pickle.load(data_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1642227722522,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"uFUu1izSBVIy","outputId":"60bf3b79-9228-403f-a7f3-93aaaec69f50"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'<': 1, '>': 2, 'a': 3, 'o': 4, 'e': 5, 'r': 6, 'i': 7, 'h': 8, 'n': 9, 't': 10, 's': 11, 'k': 12, 'u': 13, 'b': 14, 'l': 15, 'd': 16, 'm': 17, 'p': 18, 'c': 19, 'g': 20, 'j': 21, 'y': 22, 'w': 23, 'f': 24, 'v': 25, 'q': 26, 'z': 27, 'x': 28}\n","{'<': 1, '>': 2, 'া': 3, 'র': 4, 'ে': 5, 'ি': 6, '্': 7, 'ন': 8, 'ক': 9, 'স': 10, 'ব': 11, 'ল': 12, 'ম': 13, 'ত': 14, 'ু': 15, 'প': 16, 'ট': 17, 'দ': 18, 'ো': 19, 'জ': 20, 'গ': 21, 'ই': 22, 'হ': 23, 'শ': 24, 'ী': 25, 'য': 26, 'ড': 27, 'ভ': 28, 'য়': 29, 'ফ': 30, 'চ': 31, 'ও': 32, 'আ': 33, 'অ': 34, 'এ': 35, 'খ': 36, 'ষ': 37, 'ণ': 38, 'ং': 39, 'ধ': 40, 'থ': 41, 'উ': 42, 'ছ': 43, 'ূ': 44, 'ঁ': 45, 'ৃ': 46, 'ড়': 47, 'ঠ': 48, 'ঘ': 49, 'ঞ': 50, 'ঙ': 51, 'ৌ': 52, 'ৎ': 53, 'ৈ': 54, 'ঝ': 55, 'ঃ': 56, 'ঢ': 57, 'ঈ': 58, 'ঋ': 59, 'ঊ': 60, 'ঐ': 61, 'ঔ': 62, 'ঢ়': 63}\n","29 64\n"]}],"source":["vocab_inp_size = len(inp_lang_tokenizer_char.word_index) + 1\n","vocab_tar_size = len(tar_lang_tokenizer_char.word_index) + 1\n","max_length_input = 20\n","max_length_output = 20\n","\n","print(inp_lang_tokenizer_char.word_index)\n","print(tar_lang_tokenizer_char.word_index)\n","\n","embedding_dims = 32\n","rnn_units = 256\n","dense_units = 256\n","Dtype = tf.float32\n","\n","Tx = 20\n","Ty = 20\n","\n","print(vocab_inp_size, vocab_tar_size)"]},{"cell_type":"markdown","metadata":{"id":"1Swb9mvflqsQ"},"source":["### Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1642227724467,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"CJXNQ4Vh26t9","outputId":"a060e76c-6a6c-4029-c177-699ac69e9c66"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8be0504890>"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["BATCH_SIZE=64\n","class MyModel(tf.keras.Model):\n","    def __init__(self, input_vocab_size, output_vocab_size, embedding_dims, rnn_units):\n","        super().__init__()\n","        # Encoder\n","        self.input_vocab_size = input_vocab_size\n","        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n","                                                           output_dim=embedding_dims)\n","        self.encoder_rnnlayer1 = tf.keras.layers.Bidirectional(\n","            tf.keras.layers.LSTM(rnn_units, return_sequences=True))\n","        self.encoder_rnnlayer2 = tf.keras.layers.LSTM(rnn_units,\n","                                                      return_sequences=True,\n","                                                      return_state=True)\n","        self.encoder_norm = tf.keras.layers.BatchNormalization()\n","\n","        # Decoder\n","        self.output_vocab_size = output_vocab_size\n","        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n","                                                           output_dim=embedding_dims) \n","        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n","        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n","        # Sampler\n","        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n","        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n","        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n","        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, \n","                                                sampler= self.sampler,\n","                                                output_layer=self.dense_layer)\n","\n","        self.attention_mechanism.memory_initialized\n","        self.decoder_embedding_matrix = None\n","\n","\n","    def initialize_initial_state(self):\n","        self.initial_state = [\n","            tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n","\n","    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n","        return tfa.seq2seq.LuongAttention(units, \n","                                          memory = memory, \n","                                          memory_sequence_length=memory_sequence_length)\n","        # return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n","\n","    # wrap decoder rnn cell  \n","    def build_rnn_cell(self, batch_size ):\n","        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n","                                                attention_layer_size=dense_units)\n","        return rnn_cell\n","    \n","    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n","        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n","                                                                dtype = Dtype)\n","        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n","        return decoder_initial_state\n","    \n","    def call(self, inputs, training=False):\n","        encoder_input, decoder_input = inputs\n","\n","        x = self.encoder_embedding(encoder_input)\n","        x = self.encoder_rnnlayer1(x)\n","        x = self.encoder_norm(x, training=training)\n","        a, a_tx, c_tx = self.encoder_rnnlayer2(x)\n","        \n","        decoder_emb_inp = self.decoder_embedding(decoder_input)\n","        self.attention_mechanism.setup_memory(a)\n","        decoder_initial_state = self.build_decoder_initial_state(BATCH_SIZE,\n","                                                                encoder_state=[a_tx, c_tx],\n","                                                                Dtype=tf.float32)\n","        \n","        outputs, _, _ = self.decoder(decoder_emb_inp, \n","                                     initial_state=decoder_initial_state,\n","                                     sequence_length=BATCH_SIZE*[Ty-1])\n","\n","        return outputs\n","    \n","    def evaluate(self, inputs, beam_width=3):\n","        if self.decoder_embedding_matrix is None:\n","            self.decoder_embedding_matrix = tf.train.load_variable(\n","            model_weights_path, 'decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n","            # print(self.decoder_embedding_matrix.shape)\n","        \n","        inference_batch_size = inputs.shape[0]\n","        # print(inputs.shape)\n","        result = ''\n","\n","        x = self.encoder_embedding(inputs)\n","        # x = tf.one_hot(inputs, depth=self.input_vocab_size)\n","        x = self.encoder_rnnlayer1(x)\n","        x = self.encoder_norm(x, training=False)\n","        enc_out, enc_h, enc_c = self.encoder_rnnlayer2(x)\n","\n","        dec_h = enc_h\n","        # dec_c = enc_c\n","\n","        start_tokens = tf.fill([inference_batch_size], tar_lang_tokenizer_char.word_index['<'])\n","        # print(start_tokens)\n","        end_token = tar_lang_tokenizer_char.word_index['>']\n","        # print(end_token)\n","\n","        enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n","        self.attention_mechanism.setup_memory(enc_out)\n","        # print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n","\n","        # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n","        hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n","        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=beam_width * inference_batch_size,\n","                                                                dtype=tf.float32)\n","        decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n","\n","        # Instantiate BeamSearchDecoder\n","        decoder_instance = tfa.seq2seq.BeamSearchDecoder(self.rnn_cell, \n","                                                         beam_width=beam_width, \n","                                                         output_layer=self.dense_layer)\n","        decoder_instance.maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n","        # decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0]\n","\n","        # The BeamSearchDecoder object's call() function takes care of everything.\n","        outputs, final_state, sequence_lengths = decoder_instance(self.decoder_embedding_matrix, \n","                                                                  start_tokens=start_tokens,\n","                                                                  end_token=end_token, \n","                                                                  initial_state=decoder_initial_state)\n","\n","        final_outputs = tf.transpose(outputs.predicted_ids, perm=(0, 2, 1))\n","        beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0, 2, 1))\n","\n","        return final_outputs.numpy(), beam_scores.numpy()\n","\n","model = MyModel(vocab_inp_size,vocab_tar_size, embedding_dims, rnn_units)\n","model.load_weights(filepath=model_weights_path)"]},{"cell_type":"markdown","metadata":{"id":"TOJtMbjZsAuh"},"source":["### Char-level model Evaluation "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGT9KgKIIaWu"},"outputs":[],"source":["def get_tensor(tokenizer, lang):\n","    tensor = tokenizer.texts_to_sequences(lang)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n","                                                        padding='post',\n","                                                        maxlen=20, \n","                                                        truncating='post')\n","    return tensor\n","\n","\n","input_tensor_train  = get_tensor(inp_lang_tokenizer_char, inp_lang_train)\n","input_tensor_val    = get_tensor(inp_lang_tokenizer_char, inp_lang_val)\n","target_tensor_train = get_tensor(tar_lang_tokenizer_char, targ_lang_train)\n","target_tensor_val   = get_tensor(tar_lang_tokenizer_char, targ_lang_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-9Jb2QQLqut"},"outputs":[],"source":["BATCH_SIZE = 1024\n","BUFFER_SIZE = len(input_tensor_train)\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","test_dataset  = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n","test_dataset  = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQhi0irXU0Q0"},"outputs":[],"source":["# Evaluate char-level train\n","def calculate_acc(dataset):\n","    beam_width = 10\n","    correct_count = np.array([0]*4)\n","    total_count = 0\n","    steps_per_epoch = len(dataset)\n","    print(steps_per_epoch)\n","    # exit(0)\n","    start = time.time()\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        # outputs, scores = beam_evaluate(inp, beam_width=beam_width)\n","        outputs, scores = model.evaluate(inp, beam_width=beam_width)\n","        # print(targ.shape)\n","        targ = list(map(get_bangla, targ.numpy()))\n","        targ = list(map(lambda x: x.replace('<', ''), targ))\n","        # print(targ)\n","        outputs = [list(map(get_bangla, output)) for output in outputs]\n","        # print(outputs)\n","\n","        for i in range(len(targ)):\n","            if targ[i] == outputs[i][0]:\n","                correct_count[0]+=1\n","            if targ[i] in outputs[i][0:3]:\n","                correct_count[1]+=1\n","            if targ[i] in outputs[i][0:5]:\n","                correct_count[2]+=1\n","            if targ[i] in outputs[i]:\n","                correct_count[3]+=1\n","            total_count+=1\n","\n","    print(f'Total size {total_count}')\n","    print(f'Acc@1 : {((correct_count[0]/total_count))*100:.2f} %')\n","    print(f'Acc@3 : {((correct_count[1]/total_count))*100:.2f} %')\n","    print(f'Acc@5 : {((correct_count[2]/total_count))*100:.2f} %')\n","    print(f'Acc@10: {((correct_count[3]/total_count))*100:.2f} %')\n","    print(f'Time taken: {(time.time() - start):.2f} s\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbOeZmKaNglR"},"outputs":[],"source":["calculate_acc(train_dataset)\n","calculate_acc(val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"ziUlXKIiNqTc"},"source":["### Shabdik Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ltK44v2NFZq"},"outputs":[],"source":["def get_bangla(array):\n","    bangla_list = list(map(lambda x: tar_lang_tokenizer_char.index_word[x] if x != 0 else '', array))\n","    bangla_list.append('>')\n","    return \"\".join(bangla_list[0:bangla_list.index('>')])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNGAtxO8To0G"},"outputs":[],"source":["def preprocess_char(word):\n","    word = [[char for char in ('<' + word.rstrip().lstrip() + '>')]]\n","    word = inp_lang_tokenizer_char.texts_to_sequences(word)\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences(word, \n","                                                           padding='post',\n","                                                           maxlen=20, \n","                                                           truncating='post')\n","    # print(inputs)\n","    return tf.convert_to_tensor(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2Gq5sniTfX6"},"outputs":[],"source":["def predict_for_char(english_word, result, k=10):\n","    # start = time.time()\n","    outputs, score = model.evaluate(preprocess_char(english_word), k)\n","    outputs = [list(map(get_bangla, output)) for output in outputs]\n","    result[0] = outputs[0]\n","\n","    # print(f'Time taken: {(time.time() - start)*1000:.2f} ms\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1642227732591,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"bnoE_X7FK2H8","outputId":"3e96d78a-18b4-4f8c-9b33-deff37fbfe6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['আমি', 'অমি', 'এমই', 'আমী', 'আমই', 'এমি', 'অমী', 'এমী', 'মী', 'মি']\n"]}],"source":["predictions = [None]\n","predict_for_char(\"ami\", predictions)\n","print(predictions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1642227733776,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"mpZAusBPN7Uu","outputId":"2bc74246-58a1-4275-acb3-00ce66036783"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 12, 32)            928       \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 256)              164864    \n"," nal)                                                            \n","                                                                 \n"," dense_4 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                2080      \n","                                                                 \n"," dense_7 (Dense)             (None, 50001)             1650033   \n","                                                                 \n","=================================================================\n","Total params: 1,859,057\n","Trainable params: 1,859,057\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Loading word-level model\n","word_model = tf.keras.models.load_model(saved_model_word)\n","word_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nt1BPO84OWJE"},"outputs":[],"source":["# Retrieving tokenizer for word-level\n","with open(input_tokenizer_dir_word, mode='rb') as data_file:\n","    inp_lang_tokenizer_word = pickle.load(data_file)\n","with open(target_tokenizer_dir_word, mode='rb') as data_file:\n","    tar_lang_tokenizer_word = pickle.load(data_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2zrMEEwGW5C"},"outputs":[],"source":["def preprocess_word(word):\n","    word = [[char for char in ('<' + word.rstrip().lstrip() + '>')]]\n","    word = inp_lang_tokenizer_word.texts_to_sequences(word)\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences(word, \n","                                                           padding='post',\n","                                                           maxlen=12, \n","                                                           truncating='post')\n","\n","    return tf.convert_to_tensor(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsOLCJZiFc3V"},"outputs":[],"source":["def predict_for_word(input, result, k=10):\n","    if len(input) > 10:\n","        result[0] = []\n","        return\n","\n","    predictions = tf.math.top_k(tf.reshape(word_model.predict(preprocess_word(input)), [-1]), k)\n","    values = predictions.values.numpy()\n","    indices = predictions.indices.numpy()\n","\n","    for i in range(1, k):\n","        # print(values[i-1]/values[i])\n","        if values[i - 1] / values[i] > 5 or values[i] <= 0.20:\n","            result[0] = (list(map(lambda x: tar_lang_tokenizer_word.index_word[x], indices[0:i])), values[0:i])\n","            return\n","        \n","\n","    result[0] = (list(map(lambda x: tar_lang_tokenizer_word.index_word[x], indices)), values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1372,"status":"ok","timestamp":1642227739509,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"TJfxOZDFQP3I","outputId":"f9b777f7-e8e7-471d-f340-54bb989b19ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["(['কুমীর'], array([0.80083203], dtype=float32))\n"]}],"source":["predictions = [None]\n","predict_for_word(\"kumir\", predictions)\n","print(predictions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FWfdcqoOyGU"},"outputs":[],"source":["# Fixed mapping for single character word\n","single_word_dic = {\n","    'a': ['আ', 'া', 'অ', 'এ', 'ে', 'আঃ', 'া' + 'ঁ', 'আ' + 'ঁ'],\n","    'b': ['ব', 'ভ', '্' + 'ব', 'ব' + 'ি'],\n","    'c': ['চ', 'ছ', 'ক', 'ঁ', '্' + 'চ', '্' + 'ছ', 'স' + 'ি'],\n","    'd': ['ড', 'দ', 'ধ', 'দঃ', 'ডঃ', 'ঢ', '্' + 'ধ', 'ড' + 'ি', 'দ' + 'ি'],\n","    'e': ['এ', 'ে', 'আ', 'া', 'ই', 'ি', 'ঈ', 'ী', '্' + 'য' + 'া'],\n","    'f': ['ফ', 'প', '্' + 'ফ', '্' + 'প', 'এ' + 'ফ'],\n","    'g': ['গ', 'ঘ', 'য', 'জ', 'ং', 'ঞ', '্', 'জ' + 'ি'],\n","    'h': ['হ', '্', 'এ' + 'ই' + 'চ'],\n","    'i': ['ই', 'ি', 'ঈ', 'ী', 'ৈ', 'আ' + 'ই'],\n","    'j': ['জ', 'য', 'ঝ', 'জ্ব', '্' + 'জ', '্' + 'য', '্' + 'ঝ', 'জ' + 'ে'],\n","    'k': ['ক', 'খ', 'ক্ষ', '্' + 'ক', '্' + 'খ', 'ক' + 'ে'],\n","    'l': ['ল', '্' + 'ল', 'এ' + 'ল'],\n","    'm': ['ম', '্' + 'ম', 'এ' + 'ম'],\n","    'n': ['ন', 'ণ', 'ঙ', 'ঞ', 'ং', '্' + 'ন', '্' + 'ণ', '্' + 'ঙ', 'এ' + 'ন'],\n","    'o': ['ও', 'ঐ', 'ো', 'য়', 'ঃ', 'ওঁ', 'ঔ', 'ৌ', 'ৈ'],\n","    'p': ['প', 'ফ', '্' + 'প', '্' + 'ফ', 'প' + 'ি'],\n","    'q': ['ক', '্' + 'ক', 'ক' + 'ি' + 'উ'],\n","    'r': ['র', 'ঢ়', 'ড়', 'ঋ', 'ৃ', '্' + 'র', 'র' + '্', 'আ' + 'র'],\n","    's': ['স', 'শ', 'ষ', 'সঃ', '্' + 'স', '্' + 'শ', '্' + 'ষ', 'এ' + 'স'],\n","    't': ['ট', 'ঠ', 'ত', 'থ', 'তঁ', 'ৎ', '্' + 'ট', '্' + 'ঠ', 'ট' + 'ি'],\n","    'u': ['উ', 'ু', 'ঊ', 'ূ', 'উঃ', 'উঁ', 'ই' + 'উ'],\n","    'v': ['ভ', 'ব', 'ভঁ', '্' + 'ভ', '্' + 'ব', 'ভ' + 'ি'],\n","    'w': ['ও', 'ৌ', 'ওঃ', 'ওঁ', 'ড' + 'া' + 'ব' + 'ল' + 'ু'],\n","    'x': ['ক্স', 'এক্স'],\n","    'y': ['য়', 'ইয়', 'ই', 'ি', 'ঈ', 'ী', 'ে', 'ও' + 'য়' + 'া' + 'ই'],\n","    'z': ['য', 'জ', 'ঝ', '্' + 'য', '্' + 'জ', '্' + 'ঝ', 'জ' + 'এ' + 'ড', 'জ' + 'ি']\n","}\n","\n","for key in single_word_dic:\n","    for i in range(len(single_word_dic[key]), 9):\n","        single_word_dic[key].append('')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDzaMBTgPB7N"},"outputs":[],"source":["def predict(english_word, option=ALL, k=10):\n","    if type(english_word) != type(''):\n","        return []\n","    \n","    english_word = english_word.lower()\n"," \n","    if len(english_word) == 0:\n","        return []\n","\n","    # print(english_word)\n","\n","    if len(english_word) == 1:\n","        return single_word_dic[english_word][0:k]\n","\n","    word_result = [None]\n","    char_result = [None]\n","\n","    if option == WORD_LEVEL:\n","        if len(english_word) > 10:\n","            return []\n","        predict_for_word(english_word, word_result, k)\n","        # print(word_result)\n","    elif option == ENCODER_DECODER or (option == ALL and len(english_word) > 10):\n","        predict_for_char(english_word, char_result, k)\n","    else:\n","        t1 = Thread(target=predict_for_char, args=(english_word, char_result, k, ))\n","        t1.start()\n","        t2 = Thread(target=predict_for_word, args=(english_word, word_result, k, ))\n","        t2.start()\n","        t2.join()\n","        t1.join()\n","\n","    char_values = np.array([i*.1 for i in range(k)])\n","    char_values = char_values / np.sum(char_values)\n","    results = {}\n","\n","    # print(word_result[0])\n","    # print(char_result[0])\n","\n","    word_result = word_result[0]\n","    char_result = char_result[0]\n","\n","    # print(char_result)\n","    # print(word_result)\n","\n","    if word_result is not None:\n","        for i in range(len(word_result[1])):\n","            # if word_result[0][i] in results:\n","            results[word_result[0][i]] = word_result[1][i]\n","\n","    if char_result is not None:\n","        for i in range(len(char_result)):\n","            if char_result[i] in results:\n","                results[char_result[i]] += char_values[i]\n","            else:\n","                results[char_result[i]]  = char_values[i]\n","\n","    return [key for (key, value) in sorted(results.items(), key=lambda x: x[1], reverse=True)[:k]]\n"]},{"cell_type":"markdown","metadata":{"id":"b8HWcYeO1BN-"},"source":["# Ignore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_Naw1SYy5aW"},"outputs":[],"source":["char_results = np.array([0, 0, 0, 0])\n","word_results = np.array([0, 0, 0, 0])\n","combined_results = np.array([0, 0, 0, 0])\n","\n","def update_result(true, prediction, results):\n","    if prediction is None or len(prediction) == 0:\n","        return results\n","\n","    if true == prediction[0]:\n","        results[0]+=1\n","    if true in prediction[0:3]:\n","        results[1]+=1\n","    if true in prediction[0:5]:\n","        results[2]+=1\n","    if true in prediction:\n","        results[3]+=1\n","    return results\n","\n","\n","def predict(english_word, bangla_word, option=ALL, k=10):\n","    global char_results\n","    global word_results\n","    global combined_results\n","\n","    if type(english_word) != type(''):\n","        return False\n","    \n","    english_word = english_word.lower()\n"," \n","    if len(english_word) == 0:\n","        return False\n","\n","    # print(english_word)\n","\n","    if len(english_word) == 1:\n","        return single_word_dic[english_word][0:k]\n","\n","    word_result = [None]\n","    char_result = [None]\n","\n","    if option == WORD_LEVEL:\n","        if len(english_word) > 10:\n","            return []\n","        predict_for_word(english_word, word_result, k)\n","        # print(word_result)\n","    elif option == ENCODER_DECODER or (option == ALL and len(english_word) > 10):\n","        predict_for_char(english_word, char_result, k)\n","    else:\n","        t1 = Thread(target=predict_for_char, args=(english_word, char_result, k, ))\n","        t1.start()\n","        t2 = Thread(target=predict_for_word, args=(english_word, word_result, k, ))\n","        t2.start()\n","        t2.join()\n","        t1.join()\n","\n","    char_values = np.array([i*.1 for i in range(k)])\n","    char_values = char_values / np.sum(char_values)\n","    results = {}\n","\n","    # print(word_result[0])\n","    # print(char_result[0])\n","\n","    word_result = word_result[0]\n","    char_result = char_result[0]\n","\n","    # print(char_result)\n","    # print(word_result)\n","\n","    if word_result is not None:\n","        for i in range(len(word_result[1])):\n","            # if word_result[0][i] in results:\n","            results[word_result[0][i]] = word_result[1][i]\n","\n","    if char_result is not None:\n","        for i in range(len(char_result)):\n","            if char_result[i] in results:\n","                results[char_result[i]] += char_values[i]\n","            else:\n","                results[char_result[i]]  = char_values[i]\n","\n","    combined = [key for (key, value) in sorted(results.items(), key=lambda x: x[1], reverse=True)[:k]]\n","\n","#     if bangla_word == combined[0] or \\\n","#     (bangla_word in combined[0:3] or \\\n","#         bangla_word in combined[0:5] or \\\n","#    bangla_word in combined and random.random() > 0.90) or \\\n","#         random.random() > 0.90:\n","\n","            # (random.random() > .70 and bangla_word in combined[0:3]):\n","        # print(bangla_word, word_result)\n","        # word_results = update_result(bangla_word, word_result[0], word_results)\n","        # char_results = update_result(bangla_word, char_result, char_results)\n","    combined_results = update_result(bangla_word, combined, combined_results)\n","    return combined\n","    # elif bangla_word not in  char_result and word_result is not None and word_result[0] is not None:\n","    #     if bangla_word in word_result[0] or random.random() > .90:\n","    #         word_results = update_result(bangla_word, word_result[0], word_results)\n","    #         char_results = update_result(bangla_word, char_result, char_results)\n","    #         combined_results = update_result(bangla_word, combined, combined_results)\n","    #         return combined\n","    # else:\n","    #     return False"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2960818,"status":"ok","timestamp":1642232593460,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"oKFfwza8lEBx","outputId":"8e1265b6-3403-4518-a6a9-772c88d71900"},"outputs":[{"name":"stdout","output_type":"stream","text":["ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8be786dfd0>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 514, in body\n","    next_sequence_lengths,  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 870, in map_structure\n","    expand_composites=expand_composites)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 869, in <listcomp>\n","    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 506, in <lambda>\n","    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8be7921d90>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 514, in body\n","    next_sequence_lengths,  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 870, in map_structure\n","    expand_composites=expand_composites)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 869, in <listcomp>\n","    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 506, in <lambda>\n","    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8be786d550>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 514, in body\n","    next_sequence_lengths,  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 870, in map_structure\n","    expand_composites=expand_composites)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 869, in <listcomp>\n","    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 506, in <lambda>\n","    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n","combined [0.87108711 0.87548755 0.87928793 0.9439944 ]\n","Time taken for 10000 examples: 3993.47 s\n","\n"]}],"source":["random.seed(0)\n","char_results = np.array([0, 0, 0, 0])\n","word_results = np.array([0, 0, 0, 0])\n","combined_results = np.array([0, 0, 0, 0])\n","\n","start = time.time()\n","\n","# results = np.array([0, 0, 0, 0])\n","j = 0\n","lines = []\n","for i in range(len(inp_lang)):\n","    outputs = predict(inp_lang[i], targ_lang[i], k=10)\n","\n","    if outputs is None or outputs == False:\n","        continue\n","    # j += 1\n","    # lines.append(inp_lang_val[i] + ',' +targ_lang_val[i] + '\\n')\n","    # print(outputs)\n","    # print(input_lang[i])\n","    # if targ_lang_val[i] == outputs[0]:\n","    #     results[0]+=1\n","    # if targ_lang_val[i] in outputs[0:3]:\n","    #     results[1]+=1\n","    # if targ_lang_val[i] in outputs[0:5]:\n","    #     results[2]+=1\n","    # if targ_lang_val[i] in outputs:\n","    #     results[3]+=1\n","    \n","    # if j % 100 == 0:\n","    #     # print(\"char\", char_results / j)\n","    #     # print(\"word\", word_results / j)\n","    #     print(\"combined\", combined_results / j)\n","    #     print(f'Time taken for {i+1} examples: {(time.time() - start):.2f} s\\n')\n","    #     with open(test_dataset_path, mode=\"w\", encoding=\"utf-8\") as file:\n","    #         file.writelines(lines)\n","    #     if j == 10000:\n","    #         break\n","\n","# print(results / len(input_lang))\n","# print(f'Time taken: {(time.time() - start):.2f} s\\n')\n","\n","print(\"combined\", combined_results / i)\n","print(f'Time taken for {i+1} examples: {(time.time() - start):.2f} s\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hbm4RfdXKpux"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWXlnzsjRSgs"},"outputs":[],"source":["def thread_predict(input_lang, target_lang, results, thread_id, per_thread_allocation, k=10):\n","    for i in range(thread_id*per_thread_allocation, \n","                min((thread_id+1)*per_thread_allocation, len(input_lang)-1)):\n","        outputs = predict(input_lang[i], k=k)\n","        # print(outputs)\n","        # print(input_lang[i])\n","        if target_lang[i] == outputs[0]:\n","            results[thread_id][0]+=1\n","        if target_lang[i] in outputs[0:3]:\n","            results[thread_id][1]+=1\n","        if target_lang[i] in outputs[0:5]:\n","            results[thread_id][2]+=1\n","        if target_lang[i] in outputs:\n","            results[thread_id][3]+=1\n","        \n","        if i == 50:\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tmI2mowTVqa"},"outputs":[],"source":["# Evaluate word-level train\n","def calculate_acc(input_lang, target_lang):\n","    no_of_threads = 32\n","    per_thread_allocation = int(np.ceil(len(input_lang) / no_of_threads))\n","    threads = [None] * no_of_threads\n","    results = [[0, 0, 0, 0] for _ in range(no_of_threads)]\n","    for i in range(no_of_threads):\n","        x = Thread(target=thread_predict, \n","                   args=(input_lang, target_lang, results, i, per_thread_allocation, ))\n","        x.start()\n","        threads[i] = x\n","        break\n","    for i in range(no_of_threads):\n","        threads[i].join()\n","        break\n","    # print(results)\n","    print(np.sum(np.array(results), axis=0))\n","    print(np.sum(np.array(results), axis=0) / len(input_lang))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21378,"status":"ok","timestamp":1642096298544,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"TecOkQjhw7f_","outputId":"795aca1c-da1b-4341-da27-1cc766c4a4b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[12 12 14 35]\n","[2.49689968e-05 2.49689968e-05 2.91304963e-05 7.28262408e-05]\n","Time taken: 20.87 s\n","\n"]}],"source":["start = time.time()\n","calculate_acc(inp_lang_val, targ_lang_val)\n","print(f'Time taken: {(time.time() - start):.2f} s\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TOJtMbjZsAuh"],"name":"Shabdik-model.ipynb","provenance":[{"file_id":"1f_vk_fPEFOP0cef3FJRaM2fBYCb3PM_-","timestamp":1642007457660},{"file_id":"1CRne9RSF7nF5KWhrvmnANA0BnTIvwh5T","timestamp":1634998819559},{"file_id":"1UxykXamm_aEkHzDEYZIVFEt5jxtYu1cn","timestamp":1624193880157},{"file_id":"https://github.com/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb","timestamp":1621616155912}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
