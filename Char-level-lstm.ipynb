{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31543,"status":"ok","timestamp":1642129114872,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"Xo02O1JtJU5g","outputId":"bc257e17-ff05-4fb9-99fb-0e70fbe00107"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7226,"status":"ok","timestamp":1642129122092,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"G4ASKc-qyIbF","outputId":"f174205b-9aa9-4563-c723-3c8f16e129ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.15.0\n"]}],"source":["!pip install tensorflow-addons\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pickle\n","import numpy as np\n","import urllib3\n","import shutil\n","import zipfile\n","import itertools"]},{"cell_type":"markdown","metadata":{"id":"Udq3BXq10NhZ"},"source":["### Download File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-fvT1cC0Qsd"},"outputs":[],"source":["colab_base = '/content/drive/MyDrive/ashraful/'\n","pc_base = './'\n","base = colab_base\n","\n","google_dataset_path = base + 'dataset/google-dataset.txt'\n","modified_google_dataset_path = base + 'dataset/modified-google-dataset.txt'\n","phoneme_dataset = base + 'dataset/phoneme-dataset.txt'\n","my_dataset_path = base + 'dataset/my-dataset.txt'\n","new_dataset_path = base + 'dataset/new/dataset-new.txt'\n","top20k_path = base + 'dataset/new/top20k-3.txt'\n","\n","top_50k_word_file = base + 'dataset/new/top50k-sentiment.txt'\n","top_20k_word_file = base + 'dataset/new/top20k-sentiment.txt'\n","\n","input_tokenizer_retrieve = base + 'dataset/new/input-tokenizer.pickle'\n","target_tokenizer_retrieve = base + 'dataset/new/target-tokenizer.pickle'\n","target_word_tokenizer_retrieve = base + 'dataset/new/target-tokenizer-word.pickle'\n","\n","word_frequency_dict_file = base + '/dataset/new/word_frequency_dictionary.pickle'\n","word_frequency_dict_log_file = base + '/dataset/new/word_frequency_dictionary-log.pickle'\n","word_frequency_dict_sqrt_file = base + '/dataset/new/word_frequency_dictionary-sqrt.pickle'\n","\n","# dataset_paths = [new_dataset_path, top_50k_word_file, top20k_path, top20k_path]\n","dataset_paths = [new_dataset_path]\n","\n","splitted_data_path = base + 'dataset/splited-my-data-lstm'\n","\n","checkpoint_dir = base + 'models/LSTM/char-level-model-4'\n","model_weights_path = base + 'models/LSTM/char-level-model-4/weights'\n","# w-weights-2 => 47.50\n","progress_file_path = base + 'models/LSTM/progress.txt'\n","\n","\n","input_tokenizer_dir = base + 'models/LSTM/char-level-model-4/input-tokenizer.pickle'\n","target_tokenizer_dir = base + 'models/LSTM/char-level-model-4/target-tokenizer.pickle'\n","example_batch_dir = base + 'models/LSTM/example_batch.pickle'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7008,"status":"ok","timestamp":1642129163611,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"vIwO_vlG7ixY","outputId":"f16a1536-b87c-4663-f291-2c4237346b99"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1483.176439783953"]},"metadata":{},"execution_count":5}],"source":["try:\n","    with open(word_frequency_dict_sqrt_file, mode='rb') as corpus:\n","        word_frequency_dict = pickle.loads(corpus.read())\n","except:\n","    print(\"Can not open file\")\n","\n","word_frequency_dict[\"থেকে\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imc3M7Wb9BQj"},"outputs":[],"source":["class Dataset:\n","    def __init__(self):\n","        self.inp_lang_tokenizer = None\n","        self.targ_lang_tokenizer = None\n","        self.train_dataset = None\n","        self.val_dataset = None\n","\n","    def create_dataset(self):\n","        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n","        lines = list()\n","\n","        for path in dataset_paths:\n","            lines.extend(io.open(path, encoding='UTF-8').read().strip().split('\\n'))\n","        \n","        # lines = list(lines)\n","        lines.sort()\n","        print(len(lines))\n","\n","        word_pairs = [[[char for char in '<' + w.replace('ঃ\\n', '').replace('\\n', '') + '>'] for w in l.split(',')] for l in lines]\n","\n","        print(word_pairs[0][0])\n","        print(word_pairs[0][1])\n","\n","        return zip(*word_pairs)\n","\n","    # Step 3 and Step 4\n","    def tokenize(self, lang, lang_tokenizer=None, maxlen=20):\n","        if lang_tokenizer is None:\n","            lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","            lang_tokenizer.fit_on_texts(lang)\n","        \n","        tensor = lang_tokenizer.texts_to_sequences(lang)\n","        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',\n","                                                               maxlen=maxlen, truncating='post')\n","\n","        return tensor, lang_tokenizer\n","\n","    def load_dataset(self):\n","        # creating cleaned input, output pairs\n","        self.retrieve_tokenizer()\n","        inp_lang, targ_lang = self.create_dataset()\n","\n","        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang, self.inp_lang_tokenizer)\n","        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang, self.targ_lang_tokenizer)\n","\n","        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n","\n","    def save_data(self):\n","        data = tf.constant([self.inp_lang_tokenizer, self.targ_lang_tokenizer, \\\n","            self.train_dataset, self.val_dataset])\n","\n","        with open(splitted_data_path, mode='wb') as data_file:\n","            pickle.dump(data, data_file, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    def retrieve_data(self):\n","        try:\n","            1/0\n","            with open(splitted_data_path, mode='rb') as data_file:\n","                data = pickle.load(data_file)\n","                [self.inp_lang_tokenizer, self.targ_lang_tokenizer, \\\n","                    self.train_dataset, self.val_dataset] = data.numpy()\n","        except:\n","            print(\"Not found\")\n","            return False\n","\n","        return True\n","\n","    def retrieve_tokenizer(self):\n","        \n","        try:\n","            with open(input_tokenizer_retrieve, mode='rb') as data_file:\n","                self.inp_lang_tokenizer = pickle.load(data_file)\n","            \n","        except:\n","            print(\"Not found jhkhk\")\n","            return False\n","\n","        try:\n","            with open(target_tokenizer_retrieve, mode='rb') as data_file:\n","                self.targ_lang_tokenizer = pickle.load(data_file)\n","            \n","        except:\n","            print(\"Not found jgxghkjgkgjkjkjhk\")\n","            return False\n","\n","        # print(len(inp_lang_tokenizer.word_index))\n","        # print(len(targ_lang_tokenizer.word_index))\n","        return True\n","\n","    def call(self, BATCH_SIZE):\n","        # if self.retrieve_data() == False:\n","        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = \\\n","            self.load_dataset()\n","\n","        print(\"Input tensor\", input_tensor.shape)\n","        print(\"Output tensor\", target_tensor.shape)\n","\n","        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n","            train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=4651)\n","\n","        print(input_tensor_train.shape, target_tensor_train.shape)\n","        print(input_tensor_train[500])\n","        print(input_tensor_val[500])\n","\n","        BUFFER_SIZE = len(input_tensor_train)\n","        self.train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n","        self.train_dataset = self.train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","        self.val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n","        self.val_dataset = self.val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","        return self.inp_lang_tokenizer, self.targ_lang_tokenizer, self.train_dataset, self.val_dataset"]},{"cell_type":"markdown","metadata":{"id":"TOGq0hRk9tad"},"source":["### Create Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93661,"status":"ok","timestamp":1642129266067,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"oPv0BItC9wEz","outputId":"d1a4bca0-11b7-4edf-e0d0-c62cda292e9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["2402977\n","['<', 'a', '>']\n","['<', 'অ', '>']\n","Input tensor (2402977, 20)\n","Output tensor (2402977, 20)\n","(1922381, 20) (1922381, 20)\n","[ 1 20  5  8  7 14  7  2  0  0  0  0  0  0  0  0  0  0  0  0]\n","[ 1  3 27  3  9  7  3  2  0  0  0  0  0  0  0  0  0  0  0  0]\n","1877 469 28 63\n"]}],"source":["BATCH_SIZE = 1024\n","\n","dataset_creator = Dataset()\n","inp_lang, targ_lang, train_dataset, val_dataset = dataset_creator.call(BATCH_SIZE)\n","\n","print(len(train_dataset), len(val_dataset), len(inp_lang.word_index), len(targ_lang.word_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsju8WkfGHFM"},"outputs":[],"source":["with open(input_tokenizer_retrieve, mode='wb') as data_file:\n","    pickle.dump(inp_lang, data_file, protocol=pickle.HIGHEST_PROTOCOL)\n","with open(target_tokenizer_retrieve, mode='wb') as data_file:\n","    pickle.dump(targ_lang, data_file, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6668,"status":"ok","timestamp":1642129274219,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"_Le_OYP07wvl","outputId":"6c08c483-f7cc-4a37-8559-11a22f9cf9b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1024, 20) (1024, 20)\n"]}],"source":["example_input_batch, example_target_batch = next(iter(train_dataset))\n","print(example_input_batch.shape, example_target_batch.shape)\n","# print(example_input_batch[0])"]},{"cell_type":"markdown","metadata":{"id":"fLXYtWdZBW6a"},"source":["### Model Parameters"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"LBtOLOLTyY3P"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1642129321868,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"uFUu1izSBVIy","outputId":"b86e2295-018e-4d41-c60c-3fe1ee443f1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["max_length_input, max_length_target, vocab_size_input, vocab_size_target\n","20 20 29 64\n","{'<': 1, '>': 2, 'a': 3, 'o': 4, 'e': 5, 'r': 6, 'i': 7, 'h': 8, 'n': 9, 't': 10, 's': 11, 'k': 12, 'u': 13, 'b': 14, 'l': 15, 'd': 16, 'm': 17, 'p': 18, 'c': 19, 'g': 20, 'j': 21, 'y': 22, 'w': 23, 'f': 24, 'v': 25, 'q': 26, 'z': 27, 'x': 28}\n","{'<': 1, '>': 2, 'া': 3, 'র': 4, 'ে': 5, 'ি': 6, '্': 7, 'ন': 8, 'ক': 9, 'স': 10, 'ব': 11, 'ল': 12, 'ম': 13, 'ত': 14, 'ু': 15, 'প': 16, 'ট': 17, 'দ': 18, 'ো': 19, 'জ': 20, 'গ': 21, 'ই': 22, 'হ': 23, 'শ': 24, 'ী': 25, 'য': 26, 'ড': 27, 'ভ': 28, 'য়': 29, 'ফ': 30, 'চ': 31, 'ও': 32, 'আ': 33, 'অ': 34, 'এ': 35, 'খ': 36, 'ষ': 37, 'ণ': 38, 'ং': 39, 'ধ': 40, 'থ': 41, 'উ': 42, 'ছ': 43, 'ূ': 44, 'ঁ': 45, 'ৃ': 46, 'ড়': 47, 'ঠ': 48, 'ঘ': 49, 'ঞ': 50, 'ঙ': 51, 'ৌ': 52, 'ৎ': 53, 'ৈ': 54, 'ঝ': 55, 'ঃ': 56, 'ঢ': 57, 'ঈ': 58, 'ঋ': 59, 'ঊ': 60, 'ঐ': 61, 'ঔ': 62, 'ঢ়': 63}\n"]}],"source":["vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","max_length_input = example_input_batch.shape[1]\n","max_length_output = example_target_batch.shape[1]\n","\n","steps_per_epoch = len(train_dataset)//BATCH_SIZE\n","\n","print(\"max_length_input, max_length_target, vocab_size_input, vocab_size_target\")\n","print(max_length_input, max_length_output, vocab_inp_size, vocab_tar_size)\n","\n","print(inp_lang.word_index)\n","print(targ_lang.word_index)\n","\n","embedding_dims = 32\n","rnn_units = 256\n","dense_units = 256\n","Dtype = tf.float32   #used to initialize DecoderCell Zero state\n","\n","Tx = 20\n","Ty = 20"]},{"cell_type":"markdown","metadata":{"id":"DqFYB-g1HM5o"},"source":["### Creating Encoder-Decoder Model based on tfa.seq2seq module"]},{"cell_type":"markdown","metadata":{"id":"1Swb9mvflqsQ"},"source":["### Define Model"]},{"cell_type":"markdown","metadata":{"id":"wWxPNQirBtsh"},"source":["The encoder network consists of an encoder embedding layer and a LSTM layer.\n","\n","The decoder network encompasses both decoder and attention mechanism.\n","\n","The example uses LuongAttention."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1642133575323,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"CJXNQ4Vh26t9","outputId":"7bd54347-9e3e-4ff9-8ff2-7b7e3196ae7e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2a4cac05d0>"]},"metadata":{},"execution_count":26}],"source":["\n","class MyModel(tf.keras.Model):\n","    def __init__(self, input_vocab_size, output_vocab_size, embedding_dims, rnn_units):\n","        super().__init__()\n","        # Encoder\n","        self.input_vocab_size = input_vocab_size\n","        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n","                                                           output_dim=embedding_dims)\n","        self.encoder_rnnlayer1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n","        self.encoder_rnnlayer2 = tf.keras.layers.LSTM(rnn_units,\n","                                                      return_sequences=True,\n","                                                      return_state=True)\n","        self.encoder_norm = tf.keras.layers.BatchNormalization()\n","\n","        # Decoder\n","        self.output_vocab_size = output_vocab_size\n","        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n","                                                           output_dim=embedding_dims) \n","        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n","        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n","        # Sampler\n","        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n","        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n","        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n","        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, \n","                                                sampler= self.sampler,\n","                                                output_layer=self.dense_layer)\n","\n","        self.attention_mechanism.memory_initialized\n","        self.decoder_embedding_matrix = None\n","\n","\n","    def initialize_initial_state(self):\n","        self.initial_state = [\n","            tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n","\n","    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n","        return tfa.seq2seq.LuongAttention(units, \n","                                          memory = memory, \n","                                          memory_sequence_length=memory_sequence_length)\n","        # return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n","\n","    # wrap decoder rnn cell  \n","    def build_rnn_cell(self, batch_size ):\n","        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n","                                                attention_layer_size=dense_units)\n","        return rnn_cell\n","    \n","    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n","        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n","                                                                dtype = Dtype)\n","        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n","        return decoder_initial_state\n","    \n","    def call(self, inputs, training=False):\n","        encoder_input, decoder_input = inputs\n","\n","        x = self.encoder_embedding(encoder_input)\n","        x = self.encoder_rnnlayer1(x)\n","        x = self.encoder_norm(x, training=training)\n","        a, a_tx, c_tx = self.encoder_rnnlayer2(x)\n","        \n","        decoder_emb_inp = self.decoder_embedding(decoder_input)\n","        self.attention_mechanism.setup_memory(a)\n","        decoder_initial_state = self.build_decoder_initial_state(BATCH_SIZE,\n","                                                                encoder_state=[a_tx, c_tx],\n","                                                                Dtype=tf.float32)\n","        \n","        outputs, _, _ = self.decoder(decoder_emb_inp, \n","                                     initial_state=decoder_initial_state,\n","                                     sequence_length=BATCH_SIZE*[Ty-1])\n","\n","        return outputs\n","    \n","    def evaluate(self, inputs, beam_width=3):\n","        if self.decoder_embedding_matrix is None:\n","            self.decoder_embedding_matrix = tf.train.load_variable(\n","            model_weights_path, 'decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n","            print(self.decoder_embedding_matrix.shape)\n","        \n","        inference_batch_size = inputs.shape[0]\n","        # print(inputs.shape)\n","        result = ''\n","\n","        x = self.encoder_embedding(inputs)\n","        # x = tf.one_hot(inputs, depth=self.input_vocab_size)\n","        x = self.encoder_rnnlayer1(x)\n","        x = self.encoder_norm(x, training=False)\n","        enc_out, enc_h, enc_c = self.encoder_rnnlayer2(x)\n","\n","        dec_h = enc_h\n","        # dec_c = enc_c\n","\n","        start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<'])\n","        # print(start_tokens)\n","        end_token = targ_lang.word_index['>']\n","        # print(end_token)\n","\n","        enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n","        self.attention_mechanism.setup_memory(enc_out)\n","        # print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n","\n","        # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n","        hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n","        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=beam_width * inference_batch_size,\n","                                                                dtype=tf.float32)\n","        decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n","\n","        # Instantiate BeamSearchDecoder\n","        decoder_instance = tfa.seq2seq.BeamSearchDecoder(self.rnn_cell, \n","                                                         beam_width=beam_width, \n","                                                         output_layer=self.dense_layer)\n","        decoder_instance.maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n","        # decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0]\n","\n","        # The BeamSearchDecoder object's call() function takes care of everything.\n","        outputs, final_state, sequence_lengths = decoder_instance(self.decoder_embedding_matrix, \n","                                                                  start_tokens=start_tokens,\n","                                                                  end_token=end_token, \n","                                                                  initial_state=decoder_initial_state)\n","\n","        final_outputs = tf.transpose(outputs.predicted_ids, perm=(0, 2, 1))\n","        beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0, 2, 1))\n","\n","        return final_outputs.numpy(), beam_scores.numpy()\n","\n","model = MyModel(vocab_inp_size,vocab_tar_size, embedding_dims, rnn_units)\n","model.load_weights(filepath=model_weights_path)"]},{"cell_type":"markdown","metadata":{"id":"lfJTg36aCckr"},"source":["### Optimizer and Custom Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc7yp0FWEedf"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"markdown","metadata":{"id":"00WV880hoVMc"},"source":["Here, mask is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LrCHr3N9RWf"},"outputs":[],"source":["def get_bangla(array):\n","    bangla_list = list(map(lambda x: targ_lang.index_word[x] if x != 0 else '', array))\n","    bangla_list.append('>')\n","    return \"\".join(bangla_list[0:bangla_list.index('>')])\n","\n","def get_bangla_freq(array):\n","    bangla_list = list(map(lambda x: targ_lang.index_word[x] if x != 0 else '', array))\n","    bangla_list.append('>')\n","    bangla = \"\".join(bangla_list[0:bangla_list.index('>')])\n","    if bangla in word_frequency_dict:\n","        return [word_frequency_dict[bangla]]*len(array)\n","    return [1.0]*len(array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pT6hAEFEhVZ"},"outputs":[],"source":["sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(y_pred, y):\n","    bangla_freq = list(map(lambda x: get_bangla_freq(x), y.numpy()))\n","    bangla_freq = tf.convert_to_tensor(bangla_freq, dtype=y_pred.dtype)\n","    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n","    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n","    mask = tf.cast(mask, dtype=loss.dtype)\n","    loss = mask * loss * bangla_freq\n","    loss = tf.reduce_mean(loss)\n","    return loss\n","\n","\n","def acc_function(pred, real):\n","    pred = tf.reshape(pred, [pred.shape[0], 19, pred.shape[2]])\n","    pred = tf.argmax(pred, axis=2)\n","    pred = tf.cast(pred, dtype=real.dtype)\n","    pred = list(map(lambda x: get_bangla(x), pred.numpy()))\n","    real = list(map(lambda x: get_bangla(x), real.numpy()))\n","    accuracies = tf.equal(real, pred).numpy()\n","\n","    return accuracies.sum() / accuracies.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"_f8vdgaZFAah"},"source":["### One step of training on a batch using Teacher Forcing technique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nm3HkY9nExNB"},"outputs":[],"source":["\n","def train_step(input_batch, output_batch):\n","    #initialize loss = 0\n","    loss = 0\n","    acc = 0\n","\n","    with tf.GradientTape() as tape:\n","        # Prepare correct Decoder input & output sequence data\n","        decoder_input = output_batch[:,:-1] # ignore <end>\n","        #compare logits with timestepped +1 version of decoder_input\n","        decoder_output = output_batch[:,1:] #ignore <start>\n","\n","        outputs = model([input_batch, decoder_input], True)\n","\n","        logits = outputs.rnn_output\n","        #Calculate loss\n","\n","        loss = loss_function(logits, decoder_output)\n","        acc = acc_function(logits, decoder_output)\n","\n","\n","    #Returns the list of all layer variables / weights.\n","    variables = model.trainable_variables\n","    # differentiate loss wrt variables\n","    gradients = tape.gradient(loss, variables)\n","\n","    #grads_and_vars – List of(gradient, variable) pairs.\n","    grads_and_vars = zip(gradients,variables)\n","    optimizer.apply_gradients(grads_and_vars)\n","    return loss, acc"]},{"cell_type":"markdown","metadata":{"id":"Yr_GRb1JHp6b"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OT9njTi-pdB"},"outputs":[],"source":["start = 19\n","EPOCHS = 20\n","\n","dataset = train_dataset\n","steps_per_epoch = len(dataset)\n","print(steps_per_epoch)\n","max_acc = .00\n","\n","for epoch in range(start, EPOCHS+start):\n","    start = time.time()\n","\n","    # encoder_initial_cell_state = initialize_initial_state()\n","    total_loss = 0\n","    total_acc = 0\n","    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        # print(inp.shape, targ.shape)\n","        batch_loss, batch_acc = train_step(inp, targ)\n","        total_loss += batch_loss\n","        total_acc += batch_acc\n","\n","        if batch % 1000 == 0:\n","            print(f'Epoch {epoch + 1} Upto Batch {batch+1} Loss {total_loss / (batch+1):.4f} Accuracy {total_acc / (batch+1):.4f}')\n","            # model.save_weights(filepath=model_weights_path)\n","            # break\n","        \n","    # break\n","\n","    acc = total_acc / steps_per_epoch\n","    if acc > max_acc:\n","        max_acc = acc\n","        # checkpoint.save(file_prefix=checkpoint_prefix)\n","        model.save_weights(filepath=model_weights_path)\n","    else:\n","        break\n","\n","    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f} Accuracy {total_acc / steps_per_epoch:.4f}')\n","    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec\\n')"]},{"cell_type":"markdown","metadata":{"id":"TOJtMbjZsAuh"},"source":["### Evaluation"]},{"cell_type":"code","source":["# Evaluate char-level train\n","def calculate_acc(dataset):\n","    beam_width = 10\n","    correct_count = np.array([0]*4)\n","    total_count = 0\n","    steps_per_epoch = len(dataset)\n","    print(steps_per_epoch)\n","    # exit(0)\n","    start = time.time()\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        # outputs, scores = beam_evaluate(inp, beam_width=beam_width)\n","        outputs, scores = model.evaluate(inp, beam_width=beam_width)\n","        # print(targ.shape)\n","        targ = list(map(get_bangla, targ.numpy()))\n","        targ = list(map(lambda x: x.replace('<', ''), targ))\n","        # print(targ)\n","        outputs = [list(map(get_bangla, output)) for output in outputs]\n","        # print(outputs)\n","\n","        for i in range(len(targ)):\n","            if targ[i] == outputs[i][0]:\n","                correct_count[0]+=1\n","            if targ[i] in outputs[i][0:3]:\n","                correct_count[1]+=1\n","            if targ[i] in outputs[i][0:5]:\n","                correct_count[2]+=1\n","            if targ[i] in outputs[i]:\n","                correct_count[3]+=1\n","            total_count+=1\n","\n","    print(f'Total size {total_count}')\n","    print(f'Acc@1 : {((correct_count[0]/total_count))*100:.2f} %')\n","    print(f'Acc@3 : {((correct_count[1]/total_count))*100:.2f} %')\n","    print(f'Acc@5 : {((correct_count[2]/total_count))*100:.2f} %')\n","    print(f'Acc@10: {((correct_count[3]/total_count))*100:.2f} %')\n","    print(f'Time taken: {(time.time() - start):.2f} s\\n')"],"metadata":{"id":"A3EeBY6n9zQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calculate_acc(train_dataset)\n","calculate_acc(val_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmJ2IOsf99eA","executionInfo":{"status":"ok","timestamp":1642138460362,"user_tz":-360,"elapsed":4868090,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"}},"outputId":"02eaa05c-5821-4332-dd53-0d70b01851ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1877\n","(64, 32)\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2a61f01510>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 514, in body\n","    next_sequence_lengths,  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 870, in map_structure\n","    expand_composites=expand_composites)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 869, in <listcomp>\n","    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 506, in <lambda>\n","    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2a5f2fdc90>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 514, in body\n","    next_sequence_lengths,  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 870, in map_structure\n","    expand_composites=expand_composites)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 869, in <listcomp>\n","    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 506, in <lambda>\n","    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n","Total size 1922048\n","Acc@1 : 54.36 %\n","Acc@3 : 69.00 %\n","Acc@5 : 74.34 %\n","Acc@10: 80.04 %\n","Time taken: 3913.32 s\n","\n","469\n","Total size 480256\n","Acc@1 : 51.62 %\n","Acc@3 : 66.87 %\n","Acc@5 : 72.68 %\n","Acc@10: 79.01 %\n","Time taken: 954.02 s\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQhi0irXU0Q0","outputId":"f7862af4-d141-47c8-a7a4-2a80e925cee9"},"outputs":[{"output_type":"stream","name":"stdout","text":["1877\n","Upto batch: 0\n","correct: 534, Acc@1 : 52.15%\n","correct: 682, Acc@3 : 66.60%\n","correct: 751, Acc@5 : 73.34%\n","correct: 807, Acc@10: 78.81%\n","\n","Upto batch: 1000\n","correct: 528726, Acc@1 : 51.58%\n","correct: 691624, Acc@3 : 67.47%\n","correct: 752442, Acc@5 : 73.41%\n","correct: 819575, Acc@10: 79.96%\n","\n"]}],"source":["# 20k (data size 390720)\n","# Acc@1 : 68.86 %\n","# Acc@3 : 88.11 %\n","# Acc@5 : 93.14 %\n","# Acc@10: 97.05 %\n","\n","# 50k (data size 759232)\n","# Acc@1 : 71.54 %\n","# Acc@3 : 89.46 %\n","# Acc@5 : 93.96 %\n","# Acc@10: 97.40 %\n","\n","# All data (data size 2402944)\n","# Acc@1 : 51.63%\n","# Acc@3 : 67.31%\n","# Acc@5 : 73.17%\n","# Acc@10: 79.61%\n","\n","# Acc@1 : 50.63 %\n","# Acc@3 : 67.20 %\n","# Acc@5 : 73.31 %\n","# Acc@10: 79.91 %\n","\n","# Total test set size 1922304\n","# Acc@1 : 50.17 %\n","# Acc@3 : 66.50 %\n","# Acc@5 : 72.58 %\n","# Acc@10: 79.36 %\n","# Time taken: 8379.94 s\n","\n","beam_width = 10\n","correct_count = np.array([0]*4)\n","total_count = 0\n","dataset = train_dataset\n","steps_per_epoch = len(dataset)\n","print(steps_per_epoch)\n","# exit(0)\n","start = time.time()\n","for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    # outputs, scores = beam_evaluate(inp, beam_width=beam_width)\n","    outputs, scores = model.evaluate(inp, beam_width=beam_width)\n","    # print(targ.shape)\n","    targ = list(map(get_bangla, targ.numpy()))\n","    targ = list(map(lambda x: x.replace('<', ''), targ))\n","    # print(targ)\n","    outputs = [list(map(get_bangla, output)) for output in outputs]\n","    # print(outputs)\n","\n","    for i in range(len(targ)):\n","        if targ[i] == outputs[i][0]:\n","            correct_count[0]+=1\n","        if targ[i] in outputs[i][0:3]:\n","            correct_count[1]+=1\n","        if targ[i] in outputs[i][0:5]:\n","            correct_count[2]+=1\n","        if targ[i] in outputs[i]:\n","            correct_count[3]+=1\n","        total_count+=1\n","    # print(batch)\n","    if batch % 1000 == 0:\n","        print(f'Upto batch: {batch}')\n","        print(f'correct: {correct_count[0]}, Acc@1 : {((correct_count[0]/total_count))*100:.2f}%')\n","        print(f'correct: {correct_count[1]}, Acc@3 : {((correct_count[1]/total_count))*100:.2f}%')\n","        print(f'correct: {correct_count[2]}, Acc@5 : {((correct_count[2]/total_count))*100:.2f}%')\n","        print(f'correct: {correct_count[3]}, Acc@10: {((correct_count[3]/total_count))*100:.2f}%')\n","        print()\n","\n","print(f'Total test set size {total_count}')\n","print(f'Acc@1 : {((correct_count[0]/total_count))*100:.2f} %')\n","print(f'Acc@3 : {((correct_count[1]/total_count))*100:.2f} %')\n","print(f'Acc@5 : {((correct_count[2]/total_count))*100:.2f} %')\n","print(f'Acc@10: {((correct_count[3]/total_count))*100:.2f} %')\n","print(f'Time taken: {(time.time() - start):.2f} s\\n')"]},{"cell_type":"code","source":["# 20k (data size 390720)\n","# Acc@1 : 68.86 %\n","# Acc@3 : 88.11 %\n","# Acc@5 : 93.14 %\n","# Acc@10: 97.05 %\n","\n","# 50k (data size 759232)\n","# Acc@1 : 71.54 %\n","# Acc@3 : 89.46 %\n","# Acc@5 : 93.96 %\n","# Acc@10: 97.40 %\n","\n","# All data (data size 2402944)\n","# Acc@1 : 51.63%\n","# Acc@3 : 67.31%\n","# Acc@5 : 73.17%\n","# Acc@10: 79.61%\n","\n","# Acc@1 : 50.63 %\n","# Acc@3 : 67.20 %\n","# Acc@5 : 73.31 %\n","# Acc@10: 79.91 %\n","\n","# Total test set size 480512\n","# Acc@1 : 52.15 %\n","# Acc@3 : 67.86 %\n","# Acc@5 : 73.65 %\n","# Acc@10: 80.09 %\n","# Time taken: 2091.66 s\n","\n","beam_width = 10\n","correct_count = np.array([0]*4)\n","total_count = 0\n","dataset = val_dataset\n","steps_per_epoch = len(dataset)\n","print(steps_per_epoch)\n","# exit(0)\n","start = time.time()\n","for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    # outputs, scores = beam_evaluate(inp, beam_width=beam_width)\n","    outputs, scores = model.evaluate(inp, beam_width=beam_width)\n","    # print(targ.shape)\n","    targ = list(map(get_bangla, targ.numpy()))\n","    targ = list(map(lambda x: x.replace('<', ''), targ))\n","    # print(targ)\n","    outputs = [list(map(get_bangla, output)) for output in outputs]\n","    # print(outputs)\n","\n","    for i in range(len(targ)):\n","        if targ[i] == outputs[i][0]:\n","            correct_count[0]+=1\n","        if targ[i] in outputs[i][0:3]:\n","            correct_count[1]+=1\n","        if targ[i] in outputs[i][0:5]:\n","            correct_count[2]+=1\n","        if targ[i] in outputs[i]:\n","            correct_count[3]+=1\n","        total_count+=1\n","    # print(batch)\n","    if batch % 1000 == 0:\n","        print(f'Upto batch: {batch}')\n","        print(f'correct: {correct_count[0]}, Acc@1 : {((correct_count[0]/total_count))*100:.2f}%')\n","        print(f'correct: {correct_count[1]}, Acc@3 : {((correct_count[1]/total_count))*100:.2f}%')\n","        print(f'correct: {correct_count[2]}, Acc@5 : {((correct_count[2]/total_count))*100:.2f}%')\n","        print(f'correct: {correct_count[3]}, Acc@10: {((correct_count[3]/total_count))*100:.2f}%')\n","        print()\n","\n","print(f'Total test set size {total_count}')\n","print(f'Acc@1 : {((correct_count[0]/total_count))*100:.2f} %')\n","print(f'Acc@3 : {((correct_count[1]/total_count))*100:.2f} %')\n","print(f'Acc@5 : {((correct_count[2]/total_count))*100:.2f} %')\n","print(f'Acc@10: {((correct_count[3]/total_count))*100:.2f} %')\n","print(f'Time taken: {(time.time() - start):.2f} s\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIsyIT8mmQUd","executionInfo":{"status":"ok","timestamp":1642001894292,"user_tz":-360,"elapsed":230295,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"}},"outputId":"94d0a95c-1cf7-4f57-f556-999cd1a5b12a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["469\n","Upto batch: 0\n","correct: 531, Acc@1 : 51.86%\n","correct: 712, Acc@3 : 69.53%\n","correct: 767, Acc@5 : 74.90%\n","correct: 837, Acc@10: 81.74%\n","\n","Total test set size 480256\n","Acc@1 : 50.60 %\n","Acc@3 : 66.66 %\n","Acc@5 : 72.79 %\n","Acc@10: 79.60 %\n","Time taken: 685.48 s\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNGAtxO8To0G"},"outputs":[],"source":["def preprocess_word(word):\n","    word = [[char for char in ('<' + word.rstrip().lstrip() + '>')]]\n","    word = inp_lang.texts_to_sequences(word)\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences(word, padding='post',\n","                                                           maxlen=20, truncating='post')\n","    print(inputs)\n","    return tf.convert_to_tensor(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2Gq5sniTfX6"},"outputs":[],"source":["def predict(english_word):\n","    start = time.time()\n","    # outputs, score = beam_evaluate(preprocess_word(english_word), 5)\n","    outputs, score = model.evaluate(preprocess_word(english_word), 10)\n","    outputs = [list(map(get_bangla, output)) for output in outputs]\n","    print(outputs[0])\n","\n","    print(f'Time taken: {(time.time() - start)*1000:.2f} ms\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":754,"status":"ok","timestamp":1642093739325,"user":{"displayName":"DataLab CSE BUET","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12918222438203552441"},"user_tz":-360},"id":"umNWKk8gTfNC","outputId":"eb8de55d-5145-4d77-9299-9aab03339920"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1  3 17  7  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n","(64, 32)\n","tf.Tensor([1], shape=(1,), dtype=int32)\n","2\n","['আমি', 'অমি', 'এমই', 'আমী', 'আমই', 'এমি', 'অমী', 'এমী', 'মী', 'মি']\n","Time taken: 1243.29 ms\n","\n"]}],"source":["predict(\"ami\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DqFYB-g1HM5o"],"name":"Char-level-lstm.ipynb","provenance":[{"file_id":"1f_vk_fPEFOP0cef3FJRaM2fBYCb3PM_-","timestamp":1642096905172},{"file_id":"1CRne9RSF7nF5KWhrvmnANA0BnTIvwh5T","timestamp":1634998819559},{"file_id":"1UxykXamm_aEkHzDEYZIVFEt5jxtYu1cn","timestamp":1624193880157},{"file_id":"https://github.com/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb","timestamp":1621616155912}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}